# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/077_models.multimodal.ipynb.

# %% auto 0
__all__ = ['TensorSplitter', 'Embeddings', 'StaticBackbone', 'MultInputWrapper']

# %% ../../nbs/077_models.multimodal.ipynb 3
import torch
import torch.nn as nn
from fastcore.test import test_eq
from fastcore.xtras import L
from fastai.tabular.model import emb_sz_rule
from ..data.core import TSDataLoaders
from ..learner import get_arch
from .utils import build_ts_model, output_size_calculator
from .layers import Reshape, LinBnDrop, get_act_fn, lin_nd_head, rocket_nd_head

# %% ../../nbs/077_models.multimodal.ipynb 4
class TensorSplitter(nn.Module):
    def __init__(self, 
        s_cat_idxs:list=None, # list of indices for static categorical variables
        s_cont_idxs:list=None, # list of indices for static continuous variables
        o_cat_idxs:list=None, # list of indices for observed categorical variables
        o_cont_idxs:list=None, # list of indices for observed continuous variables
        k_cat_idxs:list=None, # list of indices for known categorical variables
        k_cont_idxs:list=None, # list of indices for known continuous variables
        horizon:int=None, # number of time steps to predict ahead
        ):
        super(TensorSplitter, self).__init__()
        assert s_cat_idxs or s_cont_idxs or o_cat_idxs or o_cont_idxs, "must specify at least one of s_cat_idxs, s_cont_idxs, o_cat_idxs, o_cont_idxs"
        if k_cat_idxs or k_cont_idxs:
            assert horizon is not None, "must specify horizon if using known variables"
        assert horizon is None or isinstance(horizon, int), "horizon must be an integer"
        self.s_cat_idxs = self._to_list(s_cat_idxs)
        self.s_cont_idxs = self._to_list(s_cont_idxs)
        self.o_cat_idxs = self._to_list(o_cat_idxs)
        self.o_cont_idxs = self._to_list(o_cont_idxs)
        self.k_cat_idxs = self._to_list(k_cat_idxs)
        self.k_cont_idxs = self._to_list(k_cont_idxs)
        idx_list = [self.s_cat_idxs, self.s_cont_idxs, self.o_cat_idxs, self.o_cont_idxs]
        if horizon:
            idx_list += [self.k_cat_idxs, self.k_cont_idxs]
        self.idx_list = list(map(self._to_list, idx_list))
        self._check_overlap()
        self.horizon = horizon

    def _check_overlap(self):
        indices = []
        for idx in self.idx_list:
            indices += idx
        if len(indices) != len(set(indices)):
            raise ValueError("Indices must not overlap between s_cat_idxs, s_cont_idxs, o_cat_idxs, and o_cont_idxs")

    @staticmethod
    def _to_list(idx):
        if idx is None:
            return []
        elif isinstance(idx, int):
            return [idx]
        elif isinstance(idx, list):
            return idx

    def forward(self, input_tensor):
        slices = []
        for idx, idxs in enumerate(self.idx_list):
        # for idx, idxs in enumerate([self.s_cat_idxs, self.s_cont_idxs, self.o_cat_idxs, self.o_cont_idxs, self.k_cat_idxs, self.k_cont_idxs]):
            if idxs:
                if idx < 2:  # s_cat_idxs or s_cont_idxs
                    slices.append(torch.round(input_tensor[:, idxs, 0]).long())
                elif idx < 4 and self.horizon is not None:  # o_cat_idxs or o_cont_idxs and horizon is not None
                    slices.append(input_tensor[:, idxs, :-self.horizon])
                else:  # k_cat_idxs or k_cont_idxs or o_cat_idxs or o_cont_idxs and horizon is None
                    slices.append(input_tensor[:, idxs, :])
            else:
                if idx < 2:  # s_cat_idxs or s_cont_idxs
                    slices.append(torch.empty((input_tensor.size(0), 0)))  # return 2D empty tensor
                elif idx < 4 and self.horizon is not None: # o_cat_idxs or o_cont_idxs and horizon is not None
                        slices.append(torch.empty((input_tensor.size(0), 0, input_tensor.size(2)-self.horizon)))
                else:   # k_cat_idxs or k_cont_idxs or o_cat_idxs or o_cont_idxs and horizon is None
                    slices.append(torch.empty((input_tensor.size(0), 0, input_tensor.size(2))))
        return slices


# %% ../../nbs/077_models.multimodal.ipynb 7
class Embeddings(nn.Module):
    "Embedding layers for each categorical variable in a 2D or 3D tensor"
    def __init__(self, 
        n_embeddings:list, # List of num_embeddings for each categorical variable
        embedding_dims:list=None, # List of embedding dimensions for each categorical variable
        padding_idx:int=0, # Embedding padding_idx
        embed_dropout:float=0., # Dropout probability for `Embedding` layer
        **kwargs
        ):
        super().__init__()
        if not isinstance(n_embeddings, list): n_embeddings = [n_embeddings]
        if embedding_dims is None:
            embedding_dims = [emb_sz_rule(s) for s in n_embeddings]
        if not isinstance(embedding_dims, list): embedding_dims = [embedding_dims]
        embedding_dims = [emb_sz_rule(s) if s is None else s for s in n_embeddings]
        assert len(n_embeddings) == len(embedding_dims)
        self.embedding_dims = sum(embedding_dims)
        self.embedding_layers = nn.ModuleList([nn.Sequential(nn.Embedding(n,d,padding_idx=padding_idx, **kwargs), 
                                                             nn.Dropout(embed_dropout)) for n,d in zip(n_embeddings, embedding_dims)])
    
    def forward(self, x):
        if x.ndim == 2:
            return torch.cat([e(torch.round(x[:,i]).long()) for i,e in enumerate(self.embedding_layers)],1)
        elif x.ndim == 3:
            return torch.cat([e(torch.round(x[:,i]).long()).transpose(1,2) for i,e in enumerate(self.embedding_layers)],1)

# %% ../../nbs/077_models.multimodal.ipynb 11
class StaticBackbone(nn.Module):
    "Static backbone model to embed static features"
    def __init__(self, c_in, c_out, seq_len, d=None, layers=[200, 100], dropouts=[0.1, 0.2], act=nn.ReLU(inplace=True), use_bn=False, lin_first=False):
        super().__init__()
        layers, dropouts = L(layers), L(dropouts)
        if len(dropouts) <= 1: dropouts = dropouts * len(layers)
        assert len(layers) == len(dropouts), '#layers and #dropout must match'
        self.flatten = Reshape()
        nf = [c_in * seq_len] + layers
        self.mlp = nn.ModuleList()
        for i in range(len(layers)): self.mlp.append(LinBnDrop(nf[i], nf[i+1], bn=use_bn, p=dropouts[i], act=get_act_fn(act), lin_first=lin_first))
        self.head_nf = nf[-1]

    def forward(self, x):
        x = self.flatten(x)
        for mlp in self.mlp: x = mlp(x)
        return x

# %% ../../nbs/077_models.multimodal.ipynb 13
class MultInputWrapper(nn.Module):
    "Model wrapper for input tensors with static and/ or observed, categorical and/ or numerical features."

    def __init__(self,
        arch,
        c_in:int=None, # number of input variables
        c_out:int=None, # number of output variables
        seq_len:int=None, # input sequence length
        d:tuple=None, # shape of the output tensor
        dls:TSDataLoaders=None, # TSDataLoaders object
        s_cat_idxs:list=None, # list of indices for static categorical variables
        s_cat_embeddings=None, # list of num_embeddings for each static categorical variable
        s_cat_embedding_dims=None, # list of embedding dimensions for each static categorical variable
        s_cont_idxs:list=None, # list of indices for static continuous variables
        o_cat_idxs:list=None, # list of indices for observed categorical variables
        o_cat_embeddings=None, # list of num_embeddings for each observed categorical variable
        o_cat_embedding_dims=None, # list of embedding dimensions for each observed categorical variable
        o_cont_idxs:list=None, # list of indices for observed continuous variables
        flatten=False, # boolean indicating whether to flatten bacbone's output tensor
        use_bn=False, # boolean indicating whether to use batch normalization in the head
        fc_dropout=0., # dropout probability for the fully connected layer in the head
        custom_head=None, # custom head to replace the default head
        **kwargs
    ):
        super().__init__()
        
        # attributes
        c_in = c_in or dls.vars
        c_out = c_out or dls.c
        seq_len = seq_len or dls.len
        d = d or (dls.d if dls is not None else None)
        self.c_in, self.c_out, self.seq_len, self.d = c_in, c_out, seq_len, d

        # splitter
        self.splitter = TensorSplitter(s_cat_idxs, s_cont_idxs, o_cat_idxs, o_cont_idxs)
        if c_in is not None:
            assert c_in == sum([len(self.splitter.s_cat_idxs), len(self.splitter.s_cont_idxs), len(self.splitter.o_cat_idxs), len(self.splitter.o_cont_idxs)])
        
        # embeddings
        self.s_embeddings = Embeddings(s_cat_embeddings, s_cat_embedding_dims)
        self.o_embeddings = Embeddings(o_cat_embeddings, o_cat_embedding_dims)
        
        # backbone
        n_s_features = len(self.splitter.s_cont_idxs) + self.s_embeddings.embedding_dims
        n_o_features = len(self.splitter.o_cont_idxs) + self.o_embeddings.embedding_dims
        s_backbone = StaticBackbone(c_in=n_s_features, c_out=c_out, seq_len=1, **kwargs)
        if isinstance(arch, str):
            arch = get_arch(arch)
        if isinstance(arch, nn.Module):
            o_model = arch
        else:
            o_model = build_ts_model(arch, c_in=n_o_features, c_out=c_out, seq_len=seq_len, d=d, **kwargs)
        assert hasattr(o_model, "backbone"), "the selected arch must have a backbone"
        o_backbone = getattr(o_model, "backbone")
        
        # head
        o_head_nf = output_size_calculator(o_backbone, n_o_features, seq_len)[0]
        s_head_nf = s_backbone.head_nf
        self.backbone = nn.ModuleList([o_backbone, s_backbone])
        self.head_nf = o_head_nf + s_head_nf
        if custom_head is not None: 
            if isinstance(custom_head, nn.Module): self.head = custom_head
            else:self. head = custom_head(self.head_nf, c_out, seq_len, d=d)
        else:
            if "rocket" in o_model.__name__.lower():
                self.head = rocket_nd_head(self.head_nf, c_out, seq_len=seq_len, d=d, use_bn=use_bn, fc_dropout=fc_dropout)
            else:
                self.head = lin_nd_head(self.head_nf, c_out, seq_len=seq_len, d=d, flatten=flatten, use_bn=use_bn, fc_dropout=fc_dropout)

    def forward(self, x):
        # split x into static cat, static cont, observed cat, and observed cont
        s_cat, s_cont, o_cat, o_cont = self.splitter(x)

        # create categorical embeddings
        s_cat = self.s_embeddings(s_cat)
        o_cat = self.o_embeddings(o_cat)

        # contatenate static and observed features
        s_x = torch.cat([s_cat, s_cont], 1)
        o_x = torch.cat([o_cat, o_cont], 1)
        
        # pass static and observed features through their respective backbones
        for i,(b,xi) in enumerate(zip(self.backbone, [o_x, s_x])):
            if i == 0:
                x = b(xi)
                if x.ndim == 2:
                    x = x[..., None]
            else:
                x = torch.cat([x,  b(xi)[..., None].repeat(1, 1, x.shape[-1])], 1)
        
        # head
        x = self.head(x)
        return x
