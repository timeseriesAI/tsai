{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.InceptionTimePlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionTimePlus\n",
    "\n",
    "> This is an unofficial PyTorch implementation of InceptionTime (Fawaz, 2019) created by Ignacio Oguiza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2020). [Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery, 34(6), 1936-1962.](https://arxiv.org/pdf/1909.04939)\n",
    "* Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.models.layers import *\n",
    "from tsai.models.utils import *\n",
    "torch.set_num_threads(cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com modified from:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). \n",
    "# InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "    \n",
    "class InceptionModulePlus(Module):\n",
    "    def __init__(self, ni, nf, ks=40, bottleneck=True, padding='same', coord=False, separable=False, dilation=1, stride=1, conv_dropout=0., sa=False, se=None,\n",
    "                 norm='Batch', zero_norm=False, bn_1st=True, act=nn.ReLU, act_kwargs={}):\n",
    "        if isinstance(ks, Integral): ks = [ks // (2**i) for i in range(3)]\n",
    "        ks = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in ks]  # ensure odd ks for padding='same'\n",
    "        bottleneck = False if ni == nf else bottleneck\n",
    "        self.bottleneck = Conv(ni, nf, 1, coord=coord, bias=False) if bottleneck else noop # \n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(len(ks)): self.convs.append(Conv(nf if bottleneck else ni, nf, ks[i], padding=padding, coord=coord, separable=separable,\n",
    "                                                         dilation=dilation**i, stride=stride, bias=False))\n",
    "        self.mp_conv = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv(ni, nf, 1, coord=coord, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        self.norm = Norm(nf * 4, norm=norm, zero_norm=zero_norm)\n",
    "        self.conv_dropout = nn.Dropout(conv_dropout) if conv_dropout else noop\n",
    "        self.sa = SimpleSelfAttention(nf * 4) if sa else noop\n",
    "        self.act = act(**act_kwargs) if act else noop\n",
    "        self.se = nn.Sequential(SqueezeExciteBlock(nf * 4, reduction=se), BN1d(nf * 4)) if se else noop\n",
    "        \n",
    "        self._init_cnn(self)\n",
    "    \n",
    "    def _init_cnn(self, m):\n",
    "        if getattr(self, 'bias', None) is not None: nn.init.constant_(self.bias, 0)\n",
    "        if isinstance(self, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): nn.init.kaiming_normal_(self.weight)\n",
    "        for l in m.children(): self._init_cnn(l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.mp_conv(input_tensor)])\n",
    "        x = self.norm(x)\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.sa(x)\n",
    "        x = self.act(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@delegates(InceptionModulePlus.__init__)\n",
    "class InceptionBlockPlus(Module):\n",
    "    def __init__(self, ni, nf, residual=True, depth=6, coord=False, norm='Batch', zero_norm=False, act=nn.ReLU, act_kwargs={}, sa=False, se=None, \n",
    "                 keep_prob=1., **kwargs):\n",
    "        self.residual, self.depth = residual, depth\n",
    "        self.inception, self.shortcut, self.act = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.inception.append(InceptionModulePlus(ni if d == 0 else nf * 4, nf, coord=coord, norm=norm, \n",
    "                                                      zero_norm=zero_norm if d % 3 == 2 else False,\n",
    "                                                      act=act if d % 3 != 2 else None, act_kwargs=act_kwargs, \n",
    "                                                      sa=sa if d % 3 == 2 else False,\n",
    "                                                      se=se if d % 3 != 2 else None,\n",
    "                                                      **kwargs))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                n_in, n_out = ni if d == 2 else nf * 4, nf * 4\n",
    "                self.shortcut.append(Norm(n_in, norm=norm) if n_in == n_out else ConvBlock(n_in, n_out, 1, coord=coord, bias=False, norm=norm, act=None))\n",
    "                self.act.append(act(**act_kwargs))\n",
    "        self.add = Add()\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for i in range(self.depth):\n",
    "            if self.training and self.keep_prob[i//3] < 1. and self.keep_prob[i//3] < random.random() and self.residual and i % 3 == 2: \n",
    "                res = x = self.act[i//3](self.shortcut[i//3](res))\n",
    "            else: \n",
    "                x = self.inception[i](x)\n",
    "                if self.residual and i % 3 == 2: res = x = self.act[i//3](self.add(x, self.shortcut[i//3](res)))\n",
    "        return x\n",
    "\n",
    "\n",
    "@delegates(InceptionModulePlus.__init__)\n",
    "class InceptionTimePlus(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, nf=32, nb_filters=None, concat_pool=False, fc_dropout=0., depth=6, stoch_depth=1., \n",
    "                 bn=False, y_range=None, flatten=False, custom_head=None, **kwargs):\n",
    "        \n",
    "        nf = ifnone(nf, nb_filters) # for compatibility\n",
    "        self.fc_dropout, self.c_out, self.y_range = fc_dropout, c_out, y_range\n",
    "        self.c_out = c_out\n",
    "        \n",
    "        if stoch_depth is not 0: keep_prob = np.linspace(1, stoch_depth, depth // 3)\n",
    "        else: keep_prob = np.array([1] * depth // 3)\n",
    "        backbone = InceptionBlockPlus(c_in, nf, depth=depth, keep_prob=keep_prob, **kwargs)\n",
    "        \n",
    "        #head\n",
    "        self.head_nf = nf * 4\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        if custom_head: head = custom_head(self.head_nf, c_out, seq_len)\n",
    "        else: head = self.create_head(self.head_nf, c_out, seq_len, flatten=flatten, concat_pool=concat_pool, \n",
    "                                           fc_dropout=fc_dropout, bn=bn, y_range=y_range)\n",
    "            \n",
    "        layers = OrderedDict([('backbone', nn.Sequential(backbone)), ('head', nn.Sequential(head))])\n",
    "        super().__init__(layers)\n",
    "        \n",
    "    def create_head(self, nf, c_out, seq_len, flatten=False, concat_pool=False, fc_dropout=0., bn=False, y_range=None, **kwargs):\n",
    "        if flatten: \n",
    "            nf *= seq_len\n",
    "            layers = [Flatten()]\n",
    "        else: \n",
    "            if concat_pool: nf *= 2\n",
    "            layers = [GACP1d(1) if concat_pool else GAP1d(1)]\n",
    "        layers += [LinBnDrop(nf, c_out, bn=bn, p=fc_dropout)]\n",
    "        if y_range: layers += [SigmoidRange(*y_range)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class InCoordTime(InceptionTimePlus):\n",
    "    def __init__(self, *args, coord=True, zero_norm=True, **kwargs):\n",
    "        super().__init__(*args, coord=coord, zero_norm=zero_norm, **kwargs)\n",
    "\n",
    "\n",
    "class XCoordTime(InceptionTimePlus):\n",
    "    def __init__(self, *args, coord=True, separable=True, zero_norm=True, **kwargs):\n",
    "        super().__init__(*args, coord=coord, separable=separable, zero_norm=zero_norm, **kwargs)\n",
    "        \n",
    "InceptionTimePlus17x17 = partial(InceptionTimePlus, nf=17, depth=3)\n",
    "setattr(InceptionTimePlus17x17, '__name__', 'InceptionTimePlus17x17')\n",
    "InceptionTimePlus32x32 = InceptionTimePlus\n",
    "InceptionTimePlus47x47 = partial(InceptionTimePlus, nf=47, depth=9)\n",
    "setattr(InceptionTimePlus47x47, '__name__', 'InceptionTimePlus47x47')\n",
    "InceptionTimePlus62x62 = partial(InceptionTimePlus, nf=62, depth=9)\n",
    "setattr(InceptionTimePlus62x62, '__name__', 'InceptionTimePlus62x62')\n",
    "InceptionTimeXLPlus = partial(InceptionTimePlus, nf=64, depth=12)\n",
    "setattr(InceptionTimeXLPlus, '__name__', 'InceptionTimeXLPlus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 51\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "\n",
    "test_eq(InceptionTimePlus(n_vars,c_out)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out,concat_pool=True)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, residual=False)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, conv_dropout=.5)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars, c_out, seq_len=seq_len, zero_norm=True, flatten=True)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True, \n",
    "                          norm='Instance', zero_norm=True, bn_1st=False, fc_dropout=.5, sa=True, se=True, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True,\n",
    "                          norm='Instance', zero_norm=True, bn_1st=False, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\n",
    "test_eq(total_params(InceptionTimePlus(3, 2))[0], 455490)\n",
    "test_eq(total_params(InceptionTimePlus(6, 2, **{'coord': True, 'separable': True, 'zero_norm': True}))[0], 77204)\n",
    "test_eq(total_params(InceptionTimePlus(3, 2, ks=40))[0], total_params(InceptionTimePlus(3, 2, ks=[9, 19, 39]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 51\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "\n",
    "model = InceptionTimePlus(n_vars, c_out)\n",
    "model(xb).shape\n",
    "test_eq(model[0](xb), model.backbone(xb))\n",
    "test_eq(model[1](model[0](xb)), model.head(model[0](xb)))\n",
    "test_eq(model[1].state_dict().keys(), model.head.state_dict().keys())\n",
    "test_eq(len(ts_splitter(model)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(check_bias(InceptionTimePlus(2,3, zero_norm=True), is_conv)[0].sum(), 0)\n",
    "test_eq(check_weight(InceptionTimePlus(2,3, zero_norm=True), is_bn)[0].sum(), 6)\n",
    "test_eq(check_weight(InceptionTimePlus(2,3), is_bn)[0], np.array([1., 1., 1., 1., 1., 1., 1., 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): InceptionTimePlus(n_vars,c_out,stoch_depth=0.8,depth=9,zero_norm=True)(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionTimePlus(\n",
       "  (backbone): Sequential(\n",
       "    (0): InceptionBlockPlus(\n",
       "      (inception): ModuleList(\n",
       "        (0): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (1): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (2): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (4): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (5): InceptionModulePlus(\n",
       "          (bottleneck): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): SeparableConv1d(\n",
       "                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mp_conv): Sequential(\n",
       "            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (1): ConvBlock(\n",
       "              (0): AddCoords1d()\n",
       "              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (concat): Concat(dim=1)\n",
       "          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(3, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (act): ModuleList(\n",
       "        (0): ReLU()\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (add): Add\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): GAP1d(\n",
       "        (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "        (flatten): Flatten(full=False)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): Linear(in_features=128, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = InceptionTimePlus(2,3,**{'coord': True, 'separable': True, 'zero_norm': True})\n",
    "test_eq(check_weight(net, is_bn)[0], np.array([1., 1., 0., 1., 1., 0., 1., 1.]))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(InceptionTimePlus.__init__)\n",
    "class MultiInceptionTimePlus(nn.Sequential):\n",
    "    _arch = InceptionTimePlus\n",
    "    def __init__(self, feat_list, c_out, seq_len=None, custom_head=None, device=None, **kwargs):\n",
    "        r\"\"\"\n",
    "        MultiInceptionTimePlus is a class that allows you to create a model with multiple branches of InceptionTimePlus.\n",
    "        \n",
    "        Args:\n",
    "            - feat_list: list with number of features that will be passed to each body.\n",
    "        \"\"\"\n",
    "        self.feat_list = [feat_list] if isinstance(feat_list, int) else feat_list \n",
    "        self.device = ifnone(device, default_device())\n",
    "        \n",
    "        # Backbone\n",
    "        branches = nn.ModuleList()\n",
    "        self.head_nf = 0\n",
    "        for feat in self.feat_list:\n",
    "            m = build_ts_model(self._arch, c_in=feat, c_out=c_out, seq_len=seq_len, **kwargs)\n",
    "            with torch.no_grad(): \n",
    "                self.head_nf += m[0](torch.randn(1, feat, ifnone(seq_len, 10)).to(self.device)).shape[1]\n",
    "            branches.append(m.backbone)\n",
    "        backbone = _Splitter(self.feat_list, branches)\n",
    "        \n",
    "        # Head\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        if custom_head is None:\n",
    "            head = self._arch.create_head(self, self.head_nf, c_out, seq_len, **kwargs)\n",
    "        else: \n",
    "            head = custom_head(self.head_nf, c_out, seq_len, **kwargs)\n",
    "        \n",
    "        layers = OrderedDict([('backbone', nn.Sequential(backbone)), ('head', nn.Sequential(head))])\n",
    "        super().__init__(layers)\n",
    "        self.to(self.device)\n",
    "    \n",
    "class _Splitter(Module):\n",
    "    def __init__(self, feat_list, branches):\n",
    "        self.feat_list, self.branches = feat_list, branches\n",
    "    def forward(self, x):\n",
    "        x = torch.split(x, self.feat_list, dim=1)\n",
    "        for i, branch in enumerate(self.branches):\n",
    "            out = branch(x[i]) if i == 0 else torch.cat([out, branch(x[i])], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 51\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "\n",
    "test_eq(count_parameters(MultiInceptionTimePlus([1,1,1], c_out)) > count_parameters(MultiInceptionTimePlus(3, c_out)), True)\n",
    "test_eq(MultiInceptionTimePlus([1,1,1], c_out)(xb).shape, MultiInceptionTimePlus(3, c_out)(xb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): create_conv_lin_3d_head(\n",
       "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv1d(128, 5, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (2): Transpose(-1, -2)\n",
       "    (3): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Transpose(-1, -2)\n",
       "    (5): Linear(in_features=12, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 12\n",
    "c_out = 10\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "new_head = partial(conv_lin_3d_head, d=(5,2))\n",
    "net = MultiInceptionTimePlus(n_vars, c_out, seq_len, custom_head=new_head)\n",
    "print(net(xb).shape)\n",
    "net.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): GAP1d(\n",
       "      (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "      (flatten): Flatten(full=False)\n",
       "    )\n",
       "    (1): LinBnDrop(\n",
       "      (0): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "n_vars = 6\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "net = MultiInceptionTimePlus([1,2,3], c_out, seq_len)\n",
    "print(net(xb).shape)\n",
    "net.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = build_ts_model(MultiInceptionTimePlus, [1,2,3], c_out, seq_len)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveAvgPool1d(output_size=1)\n",
       "  (1): Flatten(full=False)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Linear(in_features=128, out_features=512, bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Linear(in_features=512, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "net = MultiInceptionTimePlus(n_vars, c_out)\n",
    "change_model_head(net, create_pool_plus_head, concat_pool=False)\n",
    "print(net(xb).shape)\n",
    "net.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460038, 2822)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.learner import ts_learner\n",
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = TSStandardize()\n",
    "ts_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(ts_dls, InceptionTimePlus)\n",
    "xb,yb=first(learn.dls.train)\n",
    "test_eq(learn.model(xb).shape, (ts_dls.bs, ts_dls.c))\n",
    "p1 = count_parameters(learn.model)\n",
    "learn.freeze()\n",
    "p2 = count_parameters(learn.model)\n",
    "assert p1 > p2 > 0\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1370886, 8454)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.data.all import *\n",
    "# from tsai.learner import ts_learner\n",
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = TSStandardize()\n",
    "ts_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(ts_dls, MultiInceptionTimePlus, c_in=[4, 15, 5])\n",
    "xb,yb=first(learn.dls.train)\n",
    "test_eq(learn.model(xb).shape, (ts_dls.bs, ts_dls.c))\n",
    "p1 = count_parameters(learn.model)\n",
    "learn.freeze()\n",
    "p2 = count_parameters(learn.model)\n",
    "assert p1 > p2 > 0\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
