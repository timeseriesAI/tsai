{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.TSiTPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSiT & InceptionTSiT\n",
    "\n",
    "> These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)\n",
    "     \n",
    "Reference: \n",
    "\n",
    "     Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020).\n",
    "     An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
    "\n",
    "     This implementation is a modified version of Vision Transformer that is part of the grat timm library\n",
    "     (https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.models.layers import *\n",
    "from tsai.models.InceptionTimePlus import InceptionBlockPlus\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class _TSiTEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers:int=6, attn_drop_rate:float=0, mlp_drop_rate:float=0, drop_path_rate:float=0., \n",
    "                 mlp_ratio:int=1, qkv_bias:bool=True, act:str='reglu', pre_norm:bool=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                MultiheadAttention(d_model, n_heads, dropout=attn_drop_rate, qkv_bias=qkv_bias), nn.LayerNorm(d_model),\n",
    "                PositionwiseFeedForward(d_model, dropout=mlp_drop_rate, act=act, mlp_ratio=mlp_ratio), nn.LayerNorm(d_model),\n",
    "                # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "                DropPath(dpr[i]) if dpr[i] != 0 else nn.Identity(),\n",
    "                # nn.Dropout(drop_path_rate) if drop_path_rate != 0 else nn.Identity()\n",
    "            ]))\n",
    "        self.pre_norm = pre_norm\n",
    "        self.norm = nn.LayerNorm(d_model) if self.pre_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (mha, attn_norm, pwff, ff_norm, drop_path) in enumerate(self.layers):\n",
    "            if self.pre_norm:\n",
    "                x = drop_path(mha(attn_norm(x))[0]) + x\n",
    "                x = drop_path(pwff(ff_norm(x))) + x\n",
    "            else:\n",
    "                x = attn_norm(drop_path(mha(x)[0]) + x)\n",
    "                x = ff_norm(drop_path(pwff(x)) + x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _TSiTBackbone(Module):\n",
    "    def __init__(self, c_in:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',\n",
    "                 d_ff:int=256, qkv_bias:bool=True, pos_dropout:float=0., attn_drop_rate:float=0, mlp_drop_rate:float=0, drop_path_rate:float=0., \n",
    "                 mlp_ratio:int=1, pre_norm:bool=False, use_token:bool=True, ks:Optional[int]=None, maxpool:bool=True, \n",
    "                 preprocessor:Optional[Callable]=None, device=None, verbose:bool=False):\n",
    "\n",
    "        device = ifnone(device, default_device())\n",
    "        self.preprocessor = nn.Identity()\n",
    "        if preprocessor is not None:\n",
    "            xb = torch.randn(1, c_in, seq_len).to(device)\n",
    "            ori_c_in, ori_seq_len = c_in, seq_len\n",
    "            if not isinstance(preprocessor, nn.Module): preprocessor = preprocessor(c_in, d_model).to(device)\n",
    "            else: preprocessor = preprocessor.to(device)\n",
    "            with torch.no_grad():\n",
    "                # NOTE Most reliable way of determining output dims is to run forward pass\n",
    "                training = preprocessor.training\n",
    "                if training:\n",
    "                    preprocessor.eval()\n",
    "                c_in, seq_len = preprocessor(xb).shape[1:]\n",
    "                preprocessor.train(training)\n",
    "            pv(f'preprocessor: (?, {ori_c_in}, {ori_seq_len}) --> (?, {c_in}, {seq_len})', verbose=verbose)\n",
    "            self.preprocessor = preprocessor\n",
    "        \n",
    "        if seq_len == d_model: \n",
    "            self.to_embedding = Transpose(1,2)\n",
    "        elif ks is not None: \n",
    "            self.to_embedding = nn.Sequential(MultiConcatConv1d(c_in, d_model, kss=ks, maxpool=maxpool),Transpose(1,2))\n",
    "        else: \n",
    "            self.to_embedding = nn.Sequential(Conv1d(c_in, d_model, 1),Transpose(1,2))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.use_token = use_token\n",
    "        self.pos_dropout = nn.Dropout(pos_dropout)\n",
    "\n",
    "        self.encoder = _TSiTEncoder(d_model, n_heads, n_layers=n_layers, qkv_bias=qkv_bias, attn_drop_rate=attn_drop_rate, mlp_drop_rate=mlp_drop_rate,\n",
    "                                    mlp_ratio=mlp_ratio, drop_path_rate=drop_path_rate, act=act, pre_norm=pre_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply preprocessor module if exists\n",
    "        x = self.preprocessor(x)\n",
    "        \n",
    "        # embedding\n",
    "        x = self.to_embedding(x)\n",
    "        x = x + self.pos_embedding\n",
    "        if self.use_token:\n",
    "            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_dropout(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TSiTPlus(nn.Sequential):\n",
    "    \"\"\"Time series transformer model based on ViT (Vision Transformer):\n",
    "\n",
    "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020).\n",
    "    An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
    "\n",
    "    This implementation is a modified version of Vision Transformer that is part of the grat timm library\n",
    "    (https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py)\n",
    "\n",
    "    Args:\n",
    "    =====\n",
    "\n",
    "    c_in:                   the number of features (aka variables, dimensions, channels) in the time series dataset.\n",
    "    c_out:                  the number of target classes.\n",
    "    seq_len:                number of time steps in the time series.\n",
    "    n_layers:               number of layers (or blocks) in the encoder. Default: 3 (range(1-4))\n",
    "    d_model:                total dimension of the model (number of features created by the model). Default: 128 (range(64-512))\n",
    "    n_heads:                parallel attention heads. Default:16 (range(8-16)).\n",
    "    d_head:                 size of the learned linear projection of queries, keys and values in the MHA. Usual values: 16-512. \n",
    "                            Default: None -> (d_model/n_heads) = 32.\n",
    "    act:                    the activation function of intermediate layer, relu, gelu, geglu, reglu.\n",
    "    d_ff:                   the dimension of the feedforward network model. Default: 512 (range(256-512))\n",
    "    pos_dropout:            dropout applied to to the embedded sequence steps after position embeddings have been added.\n",
    "    attn_drop_rate (float): dropout rate applied to the attention layer\n",
    "    mlp_drop_rate (float):  dropout rate applied to the mlp layer\n",
    "    drop_path_rate:         dropout applied to the output of MultheadAttention and PositionwiseFeedForward layers.\n",
    "    mlp_ratio:              ratio of mlp hidden dim to embedding dim.\n",
    "    qkv_bias:               determines whether bias is applied to the Linear projections of queries, keys and values in the MultiheadAttention\n",
    "    pre_norm:               if True normalization will be applied as the first step in the sublayers. Defaults to False.\n",
    "    use_token:              if True, the output will come from the transformed token. Otherwise a pooling layer will be applied.\n",
    "    fc_dropout:             dropout applied to the final fully connected layer.\n",
    "    bn:                     indicates if batchnorm will be applied to the head.\n",
    "    y_range:                range of possible y values (used in regression tasks).\n",
    "    ks:                     (Optional) kernel sizes that will be applied to a hybrid embedding.\n",
    "    maxpool:                If true and kernel sizes are passed, maxpool will also be added to the hybrid embedding.\n",
    "    preprocessor:           an optional callable (nn.Conv1d with dilation > 1 or stride > 1 for example) that will be used to preprocess the time series before \n",
    "                            the embedding step. It is useful to extract features or resample the time series.\n",
    "    custom_head:            custom head that will be applied to the network. It must contain all kwargs (pass a partial function)\n",
    "\n",
    "    Input shape:\n",
    "        x: bs (batch size) x nvars (aka features, variables, dimensions, channels) x seq_len (aka time steps)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, c_in:int, c_out:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',\n",
    "                 d_ff:int=256, pos_dropout:float=0., attn_drop_rate:float=0, mlp_drop_rate:float=0, drop_path_rate:float=0., mlp_ratio:int=1,\n",
    "                 qkv_bias:bool=True, pre_norm:bool=False, use_token:bool=True, fc_dropout:float=0., bn:bool=True, y_range:Optional[tuple]=None, \n",
    "                 ks:Optional[int]=None, maxpool:bool=True, preprocessor:Optional[Callable]=None, custom_head:Optional[Callable]=None, verbose:bool=False):\n",
    "\n",
    "        backbone = _TSiTBackbone(c_in, seq_len, n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_head=d_head, act=act,\n",
    "                                          d_ff=d_ff, pos_dropout=pos_dropout, attn_drop_rate=attn_drop_rate, mlp_drop_rate=mlp_drop_rate, \n",
    "                                          drop_path_rate=drop_path_rate, pre_norm=pre_norm, mlp_ratio=mlp_ratio, use_token=use_token, \n",
    "                                          ks=ks, maxpool=maxpool, preprocessor=preprocessor, verbose=verbose)\n",
    "\n",
    "        self.head_nf = d_model\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        if custom_head: \n",
    "            head = custom_head(self.head_nf, c_out, self.seq_len) # custom head passed as a partial func with all its kwargs\n",
    "        else:\n",
    "            layers = [TokenLayer(token=use_token)]\n",
    "            layers += [LinBnDrop(d_model, c_out, bn=bn, p=fc_dropout)]\n",
    "            if y_range: layers += [SigmoidRange(*y_range)]\n",
    "            head = nn.Sequential(*layers)\n",
    "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))\n",
    "\n",
    "        \n",
    "TSiT = named_partial('TSiT', TSiTPlus)\n",
    "InceptionTSiTPlus = named_partial(\"InceptionTSiTPlus\", TSiTPlus, preprocessor=partial(InceptionBlockPlus, ks=[3,5,7]))\n",
    "InceptionTSiT = named_partial(\"InceptionTSiT\", TSiTPlus, preprocessor=partial(InceptionBlockPlus, ks=[3,5,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSiTPlus(\n",
       "  (backbone): _TSiTBackbone(\n",
       "    (preprocessor): Identity()\n",
       "    (to_embedding): Sequential(\n",
       "      (0): Conv1d(4, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): Transpose(1, 2)\n",
       "    )\n",
       "    (pos_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (encoder): _TSiTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): TokenLayer()\n",
       "    (1): LinBnDrop(\n",
       "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=128, out_features=2, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 50\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "model = TSiTPlus(nvars, c_out, seq_len)\n",
    "test_eq(model(xb).shape, (bs, c_out))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacho/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 50\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "model = InceptionTSiTPlus(nvars, c_out, seq_len)\n",
    "test_eq(model(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling\n",
    "\n",
    "It's a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x36 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TSTensor(samples:8, vars:3, len:5000, device=cpu)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.data.validation import get_splits\n",
    "from tsai.data.core import get_ts_dls\n",
    "X = np.zeros((10, 3, 5000)) \n",
    "y = np.random.randint(0,2,X.shape[0])\n",
    "splits = get_splits(y)\n",
    "dls = get_ts_dls(X, y, splits=splits)\n",
    "xb, yb = dls.train.one_batch()\n",
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to use TSiTPlus, it's likely you'll get an 'out-of-memory' error.\n",
    "\n",
    "To avoid this you can subsample the sequence reducing the input's length. This can be done in multiple ways. Here are a few examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separable convolution (to avoid mixing channels)\n",
    "preprocessor = Conv1d(xb.shape[1], xb.shape[1], ks=100, stride=50, padding='same', groups=xb.shape[1]).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution (if you want to mix channels or change number of channels)\n",
    "preprocessor = Conv1d(xb.shape[1], 2, ks=100, stride=50, padding='same').to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxPool\n",
    "preprocessor = nn.Sequential(Pad1d((0, 50), 0), nn.MaxPool1d(kernel_size=100, stride=50)).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AvgPool\n",
    "preprocessor = nn.Sequential(Pad1d((0, 50), 0), nn.AvgPool1d(kernel_size=100, stride=50)).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you decide what type of transform you want to apply, you just need to pass the layer as the preprocessor attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 1000\n",
    "c_out = 2\n",
    "d_model = 128\n",
    "\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "preprocessor = partial(Conv1d, ks=5, stride=3, padding='same', groups=xb.shape[1])\n",
    "model = TSiTPlus(nvars, c_out, seq_len, d_model=d_model, preprocessor=preprocessor)\n",
    "test_eq(model(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 000c_data.preparation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 002b_data.unwindowed.ipynb.\n",
      "Converted 002c_data.metadatasets.ipynb.\n",
      "Converted 003_data.preprocessing.ipynb.\n",
      "Converted 003b_data.transforms.ipynb.\n",
      "Converted 003c_data.mixed_augmentation.ipynb.\n",
      "Converted 003d_data.image.ipynb.\n",
      "Converted 003e_data.features.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 006_data.mixed.ipynb.\n",
      "Converted 050_losses.ipynb.\n",
      "Converted 051_metrics.ipynb.\n",
      "Converted 052_learner.ipynb.\n",
      "Converted 052b_tslearner.ipynb.\n",
      "Converted 053_optimizer.ipynb.\n",
      "Converted 060_callback.core.ipynb.\n",
      "Converted 061_callback.noisy_student.ipynb.\n",
      "Converted 063_callback.MVP.ipynb.\n",
      "Converted 064_callback.PredictionDynamics.ipynb.\n",
      "Converted 100_models.layers.ipynb.\n",
      "Converted 100b_models.utils.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 111b_models.MINIROCKET.ipynb.\n",
      "Converted 111c_models.MINIROCKET_Pytorch.ipynb.\n",
      "Converted 111d_models.MINIROCKETPlus_Pytorch.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 114b_models.XCMPlus.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 121_models.TabTransformer.ipynb.\n",
      "Converted 122_models.TabFusionTransformer.ipynb.\n",
      "Converted 123_models.TSPerceiver.ipynb.\n",
      "Converted 124_models.TSiTPlus.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 140_models.misc.ipynb.\n",
      "Converted 200_optuna.ipynb.\n",
      "Converted 201_wandb.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/Documents/Machine_Learning/Jupyter_Notebooks/tsai/tsai\n",
      "Correct conversion! ðŸ˜ƒ\n",
      "Total time elapsed 298 s\n",
      "Wednesday 22/09/21 08:02:38 CEST\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
