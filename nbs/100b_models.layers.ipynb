{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Helper function used to build PyTorch timeseries models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn.init import normal_\n",
    "from fastai.torch_core import Module\n",
    "from fastai.layers import *\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class SwishBeta(Module):\n",
    "    def __init__(self, beta=1.): \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.beta = nn.Parameter(torch.Tensor(1).fill_(beta).to(default_device()))\n",
    "    def forward(self, x): return x.mul(self.sigmoid(x*self.beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def same_padding1d(seq_len, ks, stride=1, dilation=1):\n",
    "    \"Same padding formula as used in Tensorflow\"\n",
    "    p = (seq_len - 1) * stride + (ks - 1) * dilation + 1 - seq_len\n",
    "    return p // 2, p - p // 2\n",
    "\n",
    "\n",
    "class Pad1d(nn.ConstantPad1d):\n",
    "    def __init__(self, padding, value=0.):\n",
    "        super().__init__(padding, value)\n",
    "\n",
    "\n",
    "class Conv1dSame(Module):\n",
    "    \"Conv1d with padding='same'\"\n",
    "    def __init__(self, ni, nf, ks=3, stride=1, dilation=1, **kwargs):\n",
    "        self.ks, self.stride, self.dilation = ks, stride, dilation\n",
    "        self.conv1d_same = nn.Conv1d(ni, nf, ks, stride=stride, dilation=dilation, **kwargs)\n",
    "        self.weight = self.conv1d_same.weight\n",
    "        self.bias = self.conv1d_same.bias\n",
    "        self.pad = Pad1d\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.padding = same_padding1d(x.shape[-1], self.ks, dilation=self.dilation) #stride=self.stride not used in padding calculation!\n",
    "        return self.conv1d_same(self.pad(self.padding)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_linear(Conv1dSame(2, 3, 3), None, init='auto', bias_std=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_in = 3\n",
    "c_out = 5\n",
    "seq_len = 6\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(Conv1dSame(c_in, c_out, ks=3, stride=1, dilation=1, bias=False)(t).shape, (bs, c_out, seq_len))\n",
    "test_eq(Conv1dSame(c_in, c_out, ks=3, stride=1, dilation=2, bias=False)(t).shape, (bs, c_out, seq_len))\n",
    "test_eq(Conv1dSame(c_in, c_out, ks=3, stride=2, dilation=1, bias=False)(t).shape, (bs, c_out, seq_len//2))\n",
    "test_eq(Conv1dSame(c_in, c_out, ks=3, stride=2, dilation=2, bias=False)(t).shape, (bs, c_out, seq_len//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Modified from https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\n",
    "class Conv1dCausal(Module):\n",
    "    def __init__(self, ni, nf, ks, stride=1, dilation=1, **kwargs):\n",
    "        padding = (ks - 1) * dilation\n",
    "        self.conv_causal = nn.Conv1d(ni, nf, ks, stride=stride, padding=padding, dilation=dilation, **kwargs)\n",
    "        self.weight = self.conv_causal.weight\n",
    "        self.bias = self.conv_causal.bias\n",
    "        self.chomp_size = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_causal(x)\n",
    "        return x[..., :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_linear(Conv1dCausal(2, 3, 3), None, init='auto', bias_std=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_in = 3\n",
    "c_out = 5\n",
    "seq_len = 512\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "dilation = 1\n",
    "test_eq(Conv1dCausal(c_in, c_out, ks=3, dilation=dilation)(t).shape, Conv1dSame(c_in, c_out, ks=3, dilation=dilation)(t).shape)\n",
    "dilation = 2\n",
    "test_eq(Conv1dCausal(c_in, c_out, ks=3, dilation=dilation)(t).shape, Conv1dSame(c_in, c_out, ks=3, dilation=dilation)(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(nn.Conv1d)\n",
    "def Conv1d(ni, nf, kernel_size=None, ks=None, stride=1, padding='same', dilation=1, init='auto', bias_std=0.01, **kwargs):\n",
    "    \"conv1d layer with padding='same', 'causal', 'valid', or any integer (defaults to 'same')\"\n",
    "    assert not (kernel_size and ks), 'use kernel_size or ks but not both simultaneously'\n",
    "    assert kernel_size is not None or ks is not None, 'you need to pass a ks'\n",
    "    kernel_size = kernel_size or ks\n",
    "    if padding == 'same': \n",
    "        if kernel_size%2==1: \n",
    "            conv = nn.Conv1d(ni, nf, kernel_size, stride=stride, padding=kernel_size//2 * dilation, dilation=dilation, **kwargs)\n",
    "        else:\n",
    "            conv = Conv1dSame(ni, nf, kernel_size, stride=stride, dilation=dilation, **kwargs)\n",
    "    elif padding == 'causal': conv = Conv1dCausal(ni, nf, kernel_size, stride=stride, dilation=dilation, **kwargs)\n",
    "    elif padding == 'valid': conv = nn.Conv1d(ni, nf, kernel_size, stride=stride, padding=0, dilation=dilation, **kwargs)\n",
    "    else: conv = nn.Conv1d(ni, nf, kernel_size, stride=stride, padding=padding, dilation=dilation, **kwargs)\n",
    "    init_linear(conv, None, init=init, bias_std=bias_std)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "ni = 3\n",
    "nf = 5\n",
    "seq_len = 6\n",
    "ks = 3\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(Conv1d(ni, nf, ks, padding=0)(t).shape, (bs, c_out, seq_len - (2 * (ks//2))))\n",
    "test_eq(Conv1d(ni, nf, ks, padding='valid')(t).shape, (bs, c_out, seq_len - (2 * (ks//2))))\n",
    "test_eq(Conv1d(ni, nf, ks, padding='same')(t).shape, (bs, c_out, seq_len))\n",
    "test_eq(Conv1d(ni, nf, ks, padding='causal')(t).shape, (bs, c_out, seq_len))\n",
    "test_error('use kernel_size or ks but not both simultaneously', Conv1d, ni, nf, kernel_size=3, ks=3)\n",
    "test_error('you need to pass a ks', Conv1d, ni, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(3, 5, kernel_size=(3,), stride=(1,), padding=(1,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = Conv1d(ni, nf, ks, padding='same')\n",
    "init_linear(conv, None, init='auto', bias_std=.01)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1dCausal(\n",
       "  (conv_causal): Conv1d(3, 5, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = Conv1d(ni, nf, ks, padding='causal')\n",
    "init_linear(conv, None, init='auto', bias_std=.01)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(3, 5, kernel_size=(3,), stride=(1,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = Conv1d(ni, nf, ks, padding='valid')\n",
    "init_linear(conv, None, init='auto', bias_std=.01)\n",
    "weight_norm(conv)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(3, 5, kernel_size=(3,), stride=(1,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = Conv1d(ni, nf, ks, padding=0)\n",
    "init_linear(conv, None, init='auto', bias_std=.01)\n",
    "weight_norm(conv)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeparableConv1d(Module):\n",
    "    def __init__(self, ni, nf, ks, stride=1, padding='same', dilation=1, bias=True, bias_std=0.01):\n",
    "        self.depthwise_conv = Conv1d(ni, ni, ks, stride=stride, padding=padding, dilation=dilation, groups=ni, bias=bias)\n",
    "        self.pointwise_conv = nn.Conv1d(ni, nf, 1, stride=1, padding=0, dilation=1, groups=1, bias=bias)\n",
    "        if bias:\n",
    "            if bias_std != 0: \n",
    "                normal_(self.depthwise_conv.bias, 0, bias_std)\n",
    "                normal_(self.pointwise_conv.bias, 0, bias_std)\n",
    "            else: \n",
    "                self.depthwise_conv.bias.data.zero_()\n",
    "                self.pointwise_conv.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "c_in = 6\n",
    "c_out = 5\n",
    "seq_len = 512\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(SeparableConv1d(c_in, c_out, 3)(t).shape, (bs, c_out, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddCoords1d(Module):\n",
    "    def forward(self, x):\n",
    "        bs, _, seq_len = x.size()\n",
    "        cc = torch.arange(seq_len, device=device, dtype=torch.float) / (seq_len - 1)\n",
    "        cc = cc * 2 - 1\n",
    "        cc = cc.repeat(bs, 1, 1)\n",
    "        x = torch.cat([x, cc], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_in = 3\n",
    "c_out = 5\n",
    "seq_len = 6\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(AddCoords1d()(t).shape, (bs, c_in + 1, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvBlock(nn.Sequential):\n",
    "    \"Create a sequence of conv1d (`ni` to `nf`), activation (if `act_cls`) and `norm_type` layers.\"\n",
    "    def __init__(self, ni, nf, kernel_size=None, ks=3, stride=1, padding='same', bias=None, bias_std=0.01, norm='Batch', zero_norm=False, bn_1st=True,\n",
    "                 act=nn.ReLU, act_kwargs={}, init='auto', dropout=0., xtra=None, coord=False, separable=False,  **kwargs):\n",
    "        kernel_size = kernel_size or ks\n",
    "        ndim = 1\n",
    "        layers = [AddCoords1d()] if coord else []\n",
    "        norm_type = getattr(NormType,f\"{snake2camel(norm)}{'Zero' if zero_norm else ''}\") if norm is not None else None\n",
    "        bn = norm_type in (NormType.Batch, NormType.BatchZero)\n",
    "        inn = norm_type in (NormType.Instance, NormType.InstanceZero)\n",
    "        if bias is None: bias = not (bn or inn)\n",
    "        if separable: conv = SeparableConv1d(ni + coord, nf, ks=kernel_size, bias=bias, stride=stride, padding=padding, **kwargs)\n",
    "        else: conv = Conv1d(ni + coord, nf, ks=kernel_size, bias=bias, stride=stride, padding=padding, **kwargs)\n",
    "        act = None if act is None else act(**act_kwargs)\n",
    "        if not separable: init_linear(conv, act, init=init, bias_std=bias_std)\n",
    "        if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n",
    "        elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n",
    "        layers += [conv]\n",
    "        act_bn = []        \n",
    "        if act is not None: act_bn.append(act)\n",
    "        if bn: act_bn.append(BatchNorm(nf, norm_type=norm_type, ndim=ndim))\n",
    "        if inn: act_bn.append(InstanceNorm(nf, norm_type=norm_type, ndim=ndim))\n",
    "        if bn_1st: act_bn.reverse()\n",
    "        if dropout: layers += [nn.Dropout(dropout)]\n",
    "        layers += act_bn\n",
    "        if xtra: layers.append(xtra)\n",
    "        super().__init__(*layers)     \n",
    "                            \n",
    "Conv = partial(ConvBlock, norm=None, act=None)\n",
    "ConvBN = partial(ConvBlock, norm='Batch', act=None)\n",
    "ConvIN = partial(ConvBlock, norm='Instance', act=None)\n",
    "CoordConv = partial(ConvBlock, norm=None, act=None, coord=True)\n",
    "CoordConvBN = partial(ConvBlock, norm='Batch', act=None, coord=True)\n",
    "SepConv = partial(ConvBlock, norm=None, act=None, separable=True)\n",
    "SepConvBN = partial(ConvBlock, norm='Batch', act=None, separable=True)\n",
    "SepConvIN = partial(ConvBlock, norm='Instance', act=None, separable=True)\n",
    "SepCoordConv = partial(ConvBlock, norm=None, act=None, coord=True, separable=True)\n",
    "SepCoordConvBN = partial(ConvBlock, norm='Batch', act=None, coord=True, separable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock1dPlus(Module):\n",
    "    \"Resnet block from `ni` to `nh` with `stride`\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, expansion, ni, nf, coord=False, stride=1, groups=1, reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n",
    "                 sa=False, sym=False, norm='Batch', zero_norm=True, act_cls=defaults.activation, ks=3,\n",
    "                 pool=AvgPool, pool_first=True, **kwargs):\n",
    "        if nh2 is None: nh2 = nf\n",
    "        if nh1 is None: nh1 = nh2\n",
    "        nf,ni = nf*expansion,ni*expansion\n",
    "        k0 = dict(norm=norm, zero_norm=False, act=act_cls, **kwargs)\n",
    "        k1 = dict(norm=norm, zero_norm=zero_norm, act=None, **kwargs)\n",
    "        convpath  = [ConvBlock(ni,  nh2, ks, coord=coord, stride=stride, groups=ni if dw else groups, **k0),\n",
    "                     ConvBlock(nh2,  nf, ks, coord=coord, groups=g2, **k1)\n",
    "        ] if expansion == 1 else [\n",
    "                     ConvBlock(ni,  nh1, 1, coord=coord, **k0),\n",
    "                     ConvBlock(nh1, nh2, ks, coord=coord, stride=stride, groups=nh1 if dw else groups, **k0),\n",
    "                     ConvBlock(nh2,  nf, 1, coord=coord, groups=g2, **k1)]\n",
    "        if reduction: convpath.append(SEModule(nf, reduction=reduction, act_cls=act_cls))\n",
    "        if sa: convpath.append(SimpleSelfAttention(nf,ks=1,sym=sym))\n",
    "        self.convpath = nn.Sequential(*convpath)\n",
    "        idpath = []\n",
    "        if ni!=nf: idpath.append(ConvBlock(ni, nf, 1, coord=coord, act=None, **kwargs))\n",
    "        if stride!=1: idpath.insert((1,0)[pool_first], pool(stride, ndim=1, ceil_mode=True))\n",
    "        self.idpath = nn.Sequential(*idpath)\n",
    "        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "\n",
    "    def forward(self, x): return self.act(self.convpath(x) + self.idpath(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SEModule1d(ni, reduction=16, act=nn.ReLU, act_kwargs={}):\n",
    "    \"Squeeze and excitation module for 1d\"\n",
    "    nf = math.ceil(ni//reduction/8)*8\n",
    "    assert nf != 0, 'nf cannot be 0'\n",
    "    return SequentialEx(nn.AdaptiveAvgPool1d(1), \n",
    "                        ConvBlock(ni, nf, ks=1, norm=None, act=act, act_kwargs=act_kwargs),\n",
    "                        ConvBlock(nf, ni, ks=1, norm=None, act=nn.Sigmoid), ProdLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(8, 32, 12)\n",
    "test_eq(SEModule1d(t.shape[1], 16, act=nn.ReLU, act_kwargs={})(t).shape, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Norm(nf, ndim=1, norm='Batch', zero_norm=False, init=True, **kwargs):\n",
    "    \"Norm layer with `nf` features and `ndim` with auto init.\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    nl = getattr(nn, f\"{snake2camel(norm)}Norm{ndim}d\")(nf, **kwargs)\n",
    "    if nl.affine and init:\n",
    "        nl.bias.data.fill_(1e-3)\n",
    "        nl.weight.data.fill_(0. if zero_norm else 1.)\n",
    "    return nl\n",
    "\n",
    "BN1d = partial(Norm, ndim=1, norm='Batch')\n",
    "IN1d = partial(Norm, ndim=1, norm='Instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "ni = 3\n",
    "nf = 5\n",
    "sl = 4\n",
    "ks = 5\n",
    "\n",
    "t = torch.rand(bs, ni, sl)\n",
    "test_eq(ConvBlock(ni, nf, ks)(t).shape, (bs, nf, sl))\n",
    "test_eq(ConvBlock(ni, nf, ks, padding='causal')(t).shape, (bs, nf, sl))\n",
    "test_eq(ConvBlock(ni, nf, ks, coord=True)(t).shape, (bs, nf, sl))\n",
    "ConvBlock(ni, nf, ks, stride=2)(t).shape\n",
    "test_eq(ConvBlock(ni, nf, ks, stride=2)(t).shape, (bs, nf, sl//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(BN1d(ni)(t).shape, (bs, ni, sl))\n",
    "test_eq(BN1d(ni).weight.data.mean().item(), 1.)\n",
    "test_eq(BN1d(ni, zero_norm=True).weight.data.mean().item(), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (0): AddCoords1d()\n",
       "  (1): Conv1d(4, 5, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "  (2): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Swish()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eq(ConvBlock(ni, nf, ks, norm='batch', zero_norm=True)[1].weight.data.unique().item(), 0)\n",
    "test_ne(ConvBlock(ni, nf, ks, norm='batch', zero_norm=False)[1].weight.data.unique().item(), 0)\n",
    "test_eq(ConvBlock(ni, nf, ks, bias=False)[0].bias, None)\n",
    "ConvBlock(ni, nf, ks, act=Swish, coord=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LambdaPlus(Module):\n",
    "    def __init__(self, func, *args, **kwargs): self.func,self.args,self.kwargs=func,args,kwargs\n",
    "    def forward(self, x): return self.func(x, *self.args, **self.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Squeeze(Module):\n",
    "    def __init__(self, dim=-1): self.dim = dim\n",
    "    def forward(self, x): return x.squeeze(dim=self.dim)\n",
    "    def __repr__(self): return f'{self.__class__.__name__}({self.dim})'\n",
    "\n",
    "\n",
    "class Unsqueeze(Module):\n",
    "    def __init__(self, dim=-1): self.dim = dim\n",
    "    def forward(self, x): return x.unsqueeze(dim=self.dim)\n",
    "    def __repr__(self): return f'{self.__class__.__name__}({self.dim})'\n",
    "\n",
    "\n",
    "class Add(Module):\n",
    "    def forward(self, x, y): return x.add(y)\n",
    "    def __repr__(self): return f'{self.__class__.__name__}'\n",
    "\n",
    "\n",
    "class Concat(Module):\n",
    "    def __init__(self, dim=1): self.dim = dim\n",
    "    def forward(self, *x): return torch.cat(*x, dim=self.dim)\n",
    "    def __repr__(self): return f'{self.__class__.__name__}({self.dim})'\n",
    "\n",
    "\n",
    "class Permute(Module):\n",
    "    def __init__(self, *dims): self.dims = dims\n",
    "    def forward(self, x): return x.permute(self.dims)\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({', '.join([str(d) for d in self.dims])})\"\n",
    "    \n",
    "    \n",
    "class Transpose(Module):\n",
    "    def __init__(self, *dims, contiguous=False): self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x): \n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "    def __repr__(self): \n",
    "        if self.contiguous: return f\"{self.__class__.__name__}({', '.join([str(d) for d in self.dims])}).contiguous()\"\n",
    "        else: return f\"{self.__class__.__name__}({', '.join([str(d) for d in self.dims])})\"\n",
    "    \n",
    "    \n",
    "class View(Module):\n",
    "    def __init__(self, *shape): self.shape = shape\n",
    "    def forward(self, x): return x.view(x.shape[0], *self.shape)\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({', '.join(['bs'] + [str(s) for s in self.shape])})\"\n",
    "    \n",
    "    \n",
    "class Reshape(Module):\n",
    "    def __init__(self, *shape): self.shape = shape\n",
    "    def forward(self, x): return x.reshape(x.shape[0], *self.shape)\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({', '.join(['bs'] + [str(s) for s in self.shape])})\"\n",
    "    \n",
    "    \n",
    "class Max(Module):\n",
    "    def __init__(self, dim=None, keepdim=False): self.dim, self.keepdim = dim, keepdim\n",
    "    def forward(self, x): return x.max(self.dim, keepdim=self.keepdim)[0]\n",
    "    def __repr__(self): return f'{self.__class__.__name__}({self.dim}, keepdim={self.keepdim})'\n",
    "    \n",
    "    \n",
    "class LastStep(Module):\n",
    "    def forward(self, x): return x[:, -1]\n",
    "    def __repr__(self): return f'{self.__class__.__name__}()'\n",
    "\n",
    "    \n",
    "Noop = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Transpose(1, 2),\n",
       " Permute(0, 2, 1),\n",
       " View(bs, -1, 2, 10),\n",
       " Transpose(1, 2).contiguous(),\n",
       " Reshape(bs, -1, 2, 10),\n",
       " Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 2\n",
    "nf = 5\n",
    "sl = 4\n",
    "\n",
    "t = torch.rand(bs, nf, sl)\n",
    "test_eq(Permute(0,2,1)(t).shape, (bs, sl, nf))\n",
    "test_eq(Max(1)(t).shape, (bs, sl))\n",
    "test_eq(Transpose(1,2)(t).shape, (bs, sl, nf))\n",
    "test_eq(Transpose(1,2, contiguous=True)(t).shape, (bs, sl, nf))\n",
    "test_eq(View(-1, 2, 10)(t).shape, (bs, 1, 2, 10))\n",
    "test_eq(Reshape(-1, 2, 10)(t).shape, (bs, 1, 2, 10))\n",
    "Transpose(1,2), Permute(0,2,1), View(-1, 2, 10), Transpose(1,2, contiguous=True), Reshape(-1, 2, 10), Noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Sharpen(Module):\n",
    "    \"This is used to increase confidence in predictions - MixMatch paper\"\n",
    "    def __init__(self, T=.5): self.T = T\n",
    "    def forward(self, x):\n",
    "        x = x**(1. / self.T)\n",
    "        return x / x.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "n_classes = 3\n",
    "\n",
    "t = (torch.rand(n_samples, n_classes) - .5) * 10\n",
    "probas = F.softmax(t, -1)\n",
    "sharpened_probas = Sharpen()(probas)\n",
    "plt.plot(probas.flatten().sort().values, color='r')\n",
    "plt.plot(sharpened_probas.flatten().sort().values, color='b')\n",
    "plt.show()\n",
    "test_gt(sharpened_probas[n_samples//2:].max(-1).values.sum().item(), probas[n_samples//2:].max(-1).values.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaxPPVPool1d(Module):\n",
    "    \"Drop-in replacement for AdaptiveConcatPool1d - multiplies nf by 2\"\n",
    "    def forward(self, x):\n",
    "        _max = x.max(dim=-1).values\n",
    "        _ppv = torch.gt(x, 0).sum(dim=-1).float() / x.shape[-1]\n",
    "        return torch.cat((_max, _ppv), dim=-1).unsqueeze(2)\n",
    "    \n",
    "    \n",
    "class MPPV1d(Module):\n",
    "    \"MaxPPVPool1d + Flatten\"\n",
    "    def __init__(self):\n",
    "        self.mppv = MaxPPVPool1d()\n",
    "        self.flatten = Flatten(full=False)\n",
    "    def forward(self, x):\n",
    "        return self.flatten(self.mppv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "nf = 5\n",
    "sl = 4\n",
    "\n",
    "t = torch.rand(bs, nf, sl)\n",
    "test_eq(MaxPPVPool1d()(t).shape, (bs, nf*2, 1))\n",
    "test_eq(MaxPPVPool1d()(t).shape, AdaptiveConcatPool1d(1)(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Temp_Scale(Module):\n",
    "    \"Used to perform Temperature Scaling (dirichlet=False) or Single-parameter Dirichlet calibration (dirichlet=True)\"\n",
    "    def __init__(self, temp=1., dirichlet=False):\n",
    "        self.weight = nn.Parameter(tensor(temp))\n",
    "        self.bias = None\n",
    "        self.log_softmax = dirichlet\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log_softmax: x = F.log_softmax(x, dim=-1)\n",
    "        return x.div(self.weight)\n",
    "\n",
    "\n",
    "class Vector_Scale(Module):\n",
    "    \"Used to perform Vector Scaling (dirichlet=False) or Diagonal Dirichlet calibration (dirichlet=True)\"\n",
    "    def __init__(self, n_classes=1, dirichlet=False):\n",
    "        self.weight = nn.Parameter(torch.ones(n_classes))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_classes))\n",
    "        self.log_softmax = dirichlet\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log_softmax: x = F.log_softmax(x, dim=-1)\n",
    "        return x.mul(self.weight).add(self.bias)\n",
    "\n",
    "\n",
    "class Matrix_Scale(Module):\n",
    "    \"Used to perform Matrix Scaling (dirichlet=False) or Dirichlet calibration (dirichlet=True)\"\n",
    "    def __init__(self, n_classes=1, dirichlet=False):\n",
    "        self.ms = nn.Linear(n_classes, n_classes)\n",
    "        self.ms.weight.data = nn.Parameter(torch.eye(n_classes))\n",
    "        nn.init.constant_(self.ms.bias.data, 0.)\n",
    "        self.weight = self.ms.weight\n",
    "        self.bias = self.ms.bias\n",
    "        self.log_softmax = dirichlet\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log_softmax: x = F.log_softmax(x, dim=-1)\n",
    "        return self.ms(x)\n",
    "    \n",
    "    \n",
    "def get_calibrator(calibrator=None, n_classes=1, **kwargs):\n",
    "    if calibrator is None or not calibrator: return noop\n",
    "    elif calibrator.lower() == 'temp': return Temp_Scale(dirichlet=False, **kwargs)\n",
    "    elif calibrator.lower() == 'vector': return Vector_Scale(n_classes=n_classes, dirichlet=False, **kwargs)\n",
    "    elif calibrator.lower() == 'matrix': return Matrix_Scale(n_classes=n_classes, dirichlet=False, **kwargs)\n",
    "    elif calibrator.lower() == 'dtemp': return Temp_Scale(dirichlet=True, **kwargs)\n",
    "    elif calibrator.lower() == 'dvector': return Vector_Scale(n_classes=n_classes, dirichlet=True, **kwargs)\n",
    "    elif calibrator.lower() == 'dmatrix': return Matrix_Scale(n_classes=n_classes, dirichlet=True, **kwargs)\n",
    "    else: assert False, f'please, select a correct calibrator instead of {calibrator}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_out = 3\n",
    "\n",
    "t = torch.rand(bs, c_out)\n",
    "for calibrator, cal_name in zip(['temp', 'vector', 'matrix'], ['Temp_Scale', 'Vector_Scale', 'Matrix_Scale']): \n",
    "    cal = get_calibrator(calibrator, n_classes=c_out)\n",
    "#     print(calibrator)\n",
    "#     print(cal.weight, cal.bias, '\\n')\n",
    "    test_eq(cal(t), t)\n",
    "    test_eq(cal.__class__.__name__, cal_name)\n",
    "for calibrator, cal_name in zip(['dtemp', 'dvector', 'dmatrix'], ['Temp_Scale', 'Vector_Scale', 'Matrix_Scale']):\n",
    "    cal = get_calibrator(calibrator, n_classes=c_out)\n",
    "#     print(calibrator)\n",
    "#     print(cal.weight, cal.bias, '\\n')\n",
    "    test_eq(cal(t), F.log_softmax(t, dim=1))\n",
    "    test_eq(cal.__class__.__name__, cal_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_out = 3\n",
    "\n",
    "t = torch.rand(bs, c_out)\n",
    "\n",
    "test_eq(Temp_Scale()(t).shape, t.shape)\n",
    "test_eq(Vector_Scale(c_out)(t).shape, t.shape)\n",
    "test_eq(Matrix_Scale(c_out)(t).shape, t.shape)\n",
    "test_eq(Temp_Scale(dirichlet=True)(t).shape, t.shape)\n",
    "test_eq(Vector_Scale(c_out, dirichlet=True)(t).shape, t.shape)\n",
    "test_eq(Matrix_Scale(c_out, dirichlet=True)(t).shape, t.shape)\n",
    "\n",
    "test_eq(Temp_Scale()(t), t)\n",
    "test_eq(Vector_Scale(c_out)(t), t)\n",
    "test_eq(Matrix_Scale(c_out)(t), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_out = 5\n",
    "\n",
    "t = torch.rand(bs, c_out)\n",
    "test_eq(Vector_Scale(c_out)(t), t)\n",
    "test_eq(Vector_Scale(c_out).weight.data, torch.ones(c_out))\n",
    "test_eq(Vector_Scale(c_out).weight.requires_grad, True)\n",
    "test_eq(type(Vector_Scale(c_out).weight), torch.nn.parameter.Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "c_out = 3\n",
    "weight = 2\n",
    "bias = 1\n",
    "\n",
    "t = torch.rand(bs, c_out)\n",
    "test_eq(Matrix_Scale(c_out)(t).shape, t.shape)\n",
    "test_eq(Matrix_Scale(c_out).weight.requires_grad, True)\n",
    "test_eq(type(Matrix_Scale(c_out).weight), torch.nn.parameter.Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GAP1d(Module):\n",
    "    \"Global Adaptive Pooling + Flatten\"\n",
    "    def __init__(self, output_size=1):\n",
    "        self.gap = nn.AdaptiveAvgPool1d(output_size)\n",
    "        self.flatten = Flatten()\n",
    "    def forward(self, x):\n",
    "        return self.flatten(self.gap(x))\n",
    "    \n",
    "    \n",
    "class GACP1d(Module):\n",
    "    \"Global AdaptiveConcatPool + Flatten\"\n",
    "    def __init__(self, output_size=1):\n",
    "        self.gacp = AdaptiveConcatPool1d(output_size)\n",
    "        self.flatten = Flatten()\n",
    "    def forward(self, x):\n",
    "        return self.flatten(self.gacp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, c_in, seq_len = 16, 1, 50\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(GAP1d()(t).shape, (bs, c_in))\n",
    "test_eq(GACP1d()(t).shape, (bs, c_in*2))\n",
    "bs, c_in, seq_len = 16, 4, 50\n",
    "t = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(GAP1d()(t).shape, (bs, c_in))\n",
    "test_eq(GACP1d()(t).shape, (bs, c_in*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SqueezeExciteBlock(Module):\n",
    "    def __init__(self, ni, reduction=16):\n",
    "        self.avg_pool = GAP1d(1)\n",
    "        self.fc = nn.Sequential(nn.Linear(ni, ni // reduction, bias=False), nn.ReLU(),  nn.Linear(ni // reduction, ni, bias=False), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.fc(y).unsqueeze(2)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "ni = 32\n",
    "sl = 4\n",
    "t = torch.rand(bs, ni, sl)\n",
    "test_eq(SqueezeExciteBlock(ni)(t).shape, (bs, ni, sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_pool_head(nf, c_out, concat_pool=False, fc_dropout=0., bn=False, y_range=None):\n",
    "    if concat_pool: nf = nf * 2\n",
    "    layers = [GACP1d(1) if concat_pool else GAP1d(1)]\n",
    "    layers += [LinBnDrop(nf, c_out, bn=bn, p=fc_dropout)]\n",
    "    if y_range: layers += [SigmoidRange(*y_range)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GACP1d(\n",
       "    (gacp): AdaptiveConcatPool1d(\n",
       "      (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "    )\n",
       "    (flatten): Flatten(full=False)\n",
       "  )\n",
       "  (1): LinBnDrop(\n",
       "    (0): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=24, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nf = 12\n",
    "c_out = 2\n",
    "seq_len = 20\n",
    "t = torch.rand(bs, nf, seq_len)\n",
    "test_eq(create_pool_head(nf, c_out, fc_dropout=0.5)(t).shape, (bs, c_out))\n",
    "test_eq(create_pool_head(nf, c_out, concat_pool=True, fc_dropout=0.5)(t).shape, (bs, c_out))\n",
    "create_pool_head(nf, c_out, concat_pool=True, bn=True, fc_dropout=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_pool_plus_head(nf, c_out, lin_ftrs=None, fc_dropout=0., concat_pool=True, bn_final=False, lin_first=False, y_range=None):\n",
    "    if concat_pool: nf = nf * 2\n",
    "    lin_ftrs = [nf, 512, c_out] if lin_ftrs is None else [nf] + lin_ftrs + [c_out]\n",
    "    ps = L(fc_dropout)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    pool = AdaptiveConcatPool1d() if concat_pool else nn.AdaptiveAvgPool1d(1)\n",
    "    layers = [pool, Flatten()]\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], c_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveConcatPool1d(\n",
       "    (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (1): Flatten(full=False)\n",
       "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.25, inplace=False)\n",
       "  (4): Linear(in_features=24, out_features=512, bias=False)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nf = 12\n",
    "c_out = 2\n",
    "seq_len = 20\n",
    "t = torch.rand(bs, nf, seq_len)\n",
    "test_eq(create_pool_plus_head(nf, c_out, fc_dropout=0.5)(t).shape, (bs, c_out))\n",
    "test_eq(create_pool_plus_head(nf, c_out, concat_pool=True, fc_dropout=0.5)(t).shape, (bs, c_out))\n",
    "create_pool_plus_head(nf, c_out, fc_dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mlp_head(nf, c_out, fc_dropout=0., bn=False, y_range=None):\n",
    "    layers = [LinBnDrop(nf, c_out, bn=bn, p=fc_dropout)]\n",
    "    if y_range: layers += [SigmoidRange(*y_range)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LinBnDrop(\n",
       "    (0): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=240, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nf = 12\n",
    "c_out = 2\n",
    "seq_len = 20\n",
    "t = torch.rand(bs, nf*seq_len)\n",
    "test_eq(create_mlp_head(nf*seq_len, c_out, fc_dropout=0.5)(t).shape, (bs, c_out))\n",
    "create_mlp_head(nf*seq_len, c_out, bn=True, fc_dropout=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_conv_head(nf, c_out, adaptive_size=None, y_range=None):\n",
    "    layers = [nn.AdaptiveAvgPool1d(adaptive_size)] if adaptive_size is not None else []\n",
    "    for i in range(2):\n",
    "        if nf > 1: \n",
    "            layers += [ConvBlock(nf, nf // 2, 1)] \n",
    "            nf = nf//2\n",
    "        else: break\n",
    "    layers += [ConvBlock(nf, c_out, 1), GAP1d(1)]\n",
    "    if y_range: layers += [SigmoidRange(*y_range)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveAvgPool1d(output_size=50)\n",
       "  (1): ConvBlock(\n",
       "    (0): Conv1d(12, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (1): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (2): ConvBlock(\n",
       "    (0): Conv1d(6, 3, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): ConvBlock(\n",
       "    (0): Conv1d(3, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (4): GAP1d(\n",
       "    (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (flatten): Flatten(full=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nf = 12\n",
    "c_out = 2\n",
    "seq_len = 20\n",
    "t = torch.rand(bs, nf, seq_len)\n",
    "test_eq(create_conv_head(nf, c_out)(t).shape, (bs, c_out))\n",
    "test_eq(create_conv_head(nf, c_out, adaptive_size=50)(t).shape, (bs, c_out))\n",
    "create_conv_head(nf, c_out, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def change_model_head(model, custom_head, **kwargs):\n",
    "    r\"\"\"Replaces a model's head by a custom head as long as the model has a head and head_nf attributes\"\"\"\n",
    "    model.head = custom_head(model.head_nf, model.c_out, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GaussianNoise(Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
    "        self.sigma, self.is_relative_detach = sigma, is_relative_detach\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma not in [0, None]:\n",
    "            scale = self.sigma * (x.detach() if self.is_relative_detach else x)\n",
    "            sampled_noise = torch.empty(x.size()).normal_().to(device) * scale\n",
    "            x = x + sampled_noise\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(2,3,4)\n",
    "test_ne(GaussianNoise()(t), t)\n",
    "test_eq(GaussianNoise()(t).shape, t.shape)\n",
    "t = torch.ones(2,3)\n",
    "test_ne(GaussianNoise()(t), t)\n",
    "test_eq(GaussianNoise()(t).shape, t.shape)\n",
    "t = torch.ones(2)\n",
    "test_ne(GaussianNoise()(t), t)\n",
    "test_eq(GaussianNoise()(t).shape, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gambler_loss(reward=2):\n",
    "    def _gambler_loss(model_output, targets):\n",
    "        outputs = torch.nn.functional.softmax(model_output, dim=1)\n",
    "        outputs, reservation = outputs[:, :-1], outputs[:, -1]\n",
    "        gain = torch.gather(outputs, dim=1, index=targets.unsqueeze(1)).squeeze()\n",
    "        doubling_rate = (gain + reservation / reward).log()\n",
    "        return - doubling_rate.mean()\n",
    "    return _gambler_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7046)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = torch.rand(16, 3)\n",
    "targets = torch.randint(0, 2, (16,))\n",
    "criterion = gambler_loss(2)\n",
    "criterion(model_output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CrossEntropyLossOneHot(output, target, **kwargs):\n",
    "    if target.ndim == 2: _, target = target.max(dim=1)\n",
    "    return nn.CrossEntropyLoss(**kwargs)(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6375)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.rand(16, 2)\n",
    "target = torch.randint(0, 2, (16,))\n",
    "CrossEntropyLossOneHot(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6921, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.data.transforms import OneHot\n",
    "output = nn.Parameter(torch.rand(16, 2))\n",
    "target = torch.randint(0, 2, (16,))\n",
    "one_hot_target = OneHot()(target)\n",
    "CrossEntropyLossOneHot(output, one_hot_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def proba_certainty(output):\n",
    "    if output.sum(-1).mean().item() != 1: output = F.softmax(output, -1)\n",
    "    return (output.max(-1).values - 1. / output.shape[-1])/( 1 - 1. / output.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3792, 0.2751, 0.4989, 0.8306, 0.7965, 0.9457, 0.2849, 0.8981, 0.9376,\n",
       "        0.4672, 0.7664, 0.5925, 0.7429, 0.2126, 0.1688, 0.5495],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "target = random_shuffle(concat(torch.zeros(5), torch.ones(7), torch.ones(4) + 1)).long()\n",
    "output = nn.Parameter(5 * torch.rand((16, 3)) - 5 * torch.rand((16, 3)))\n",
    "proba_certainty(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def CrossEntropyLossOneHotWithUncertainty():\n",
    "    def _CrossEntropyLossOneHotWithUncertainty(output, target, **kwargs):\n",
    "        return (proba_certainty(output) * CrossEntropyLossOneHot(output, target, reduction='none', **kwargs)).mean()\n",
    "    return _CrossEntropyLossOneHotWithUncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttest_ind:            t = -1.5827  p = 0.118873\n",
      "ttest_ind_from_stats: t = -1.5827  p = 0.118873\n",
      "formula:              t = -1.5827  p = 0.118873\n",
      "formula:              t = -1.5827\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# https://stackoverflow.com/questions/22611446/perform-2-sample-t-test\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "from scipy.special import stdtr\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create sample data.\n",
    "a = np.random.randn(40)\n",
    "b = 4*np.random.randn(50)\n",
    "\n",
    "# Use scipy.stats.ttest_ind.\n",
    "t, p = ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind:            t = %g  p = %g\" % (t, p))\n",
    "\n",
    "# Compute the descriptive statistics of a and b.\n",
    "abar = a.mean()\n",
    "avar = a.var(ddof=1)\n",
    "na = a.size\n",
    "adof = na - 1\n",
    "\n",
    "bbar = b.mean()\n",
    "bvar = b.var(ddof=1)\n",
    "nb = b.size\n",
    "bdof = nb - 1\n",
    "\n",
    "# Use scipy.stats.ttest_ind_from_stats.\n",
    "t2, p2 = ttest_ind_from_stats(abar, np.sqrt(avar), na,\n",
    "                              bbar, np.sqrt(bvar), nb,\n",
    "                              equal_var=False)\n",
    "print(\"ttest_ind_from_stats: t = %g  p = %g\" % (t2, p2))\n",
    "\n",
    "# Use the formulas directly.\n",
    "tf = (abar - bbar) / np.sqrt(avar/na + bvar/nb)\n",
    "dof = (avar/na + bvar/nb)**2 / (avar**2/(na**2*adof) + bvar**2/(nb**2*bdof))\n",
    "pf = 2*stdtr(dof, -np.abs(tf))\n",
    "\n",
    "print(\"formula:              t = %g  p = %g\" % (tf, pf))\n",
    "\n",
    "a = tensor(a)\n",
    "b = tensor(b)\n",
    "tf = (a.mean() - b.mean()) / torch.sqrt(a.var()/a.size(0) + b.var()/b.size(0))\n",
    "print(\"formula:              t = %g\" % (tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.5827)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_tensor(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ttest_bin_loss(output, target):\n",
    "    output = nn.Softmax(dim=-1)(output[:, 1])\n",
    "    return ttest_tensor(output[target == 0], output[target == 1])\n",
    "\n",
    "def ttest_reg_loss(output, target):\n",
    "    return ttest_tensor(output[target <= 0], output[target > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    output = torch.rand(256, 2)\n",
    "    target = torch.randint(0, 2, (256,))\n",
    "    test_close(ttest_bin_loss(output, target).item(), \n",
    "               ttest_ind(nn.Softmax(dim=-1)(output[:, 1])[target == 0], nn.Softmax(dim=-1)(output[:, 1])[target == 1], equal_var=False)[0], eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CenterLoss(Module):\n",
    "    r\"\"\"\n",
    "    Code in Pytorch has been slightly modified from: https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py\n",
    "    Based on paper: Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "\n",
    "    Args:\n",
    "        c_out (int): number of classes.\n",
    "        logits_dim (int): dim 1 of the logits. By default same as c_out (for one hot encoded logits)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, c_out, logits_dim=None):\n",
    "        logits_dim = ifnone(logits_dim, c_out)\n",
    "        self.c_out, self.logits_dim = c_out, logits_dim\n",
    "        self.centers = nn.Parameter(torch.randn(c_out, logits_dim).to(device=default_device()))\n",
    "        self.classes = torch.arange(c_out).long().to(device=default_device())\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, logits_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        bs = x.shape[0]\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(bs, self.c_out) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.c_out, bs).T\n",
    "        distmat = torch.addmm(distmat, x, self.centers.T, beta=1, alpha=-2)\n",
    "\n",
    "        labels = labels.unsqueeze(1).expand(bs, self.c_out)\n",
    "        mask = labels.eq(self.classes.expand(bs, self.c_out))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / bs\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CenterPlusLoss(Module):\n",
    "    \n",
    "    def __init__(self, loss, c_out, =1e-2, logits_dim=None):\n",
    "        self.loss, self.c_out, self. = loss, c_out, \n",
    "        self.centerloss = CenterLoss(c_out, logits_dim)\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        return self.loss(x, labels) + self. * self.centerloss(x, labels)\n",
    "    def __repr__(self): return f\"CenterPlusLoss(loss={self.loss}, c_out={self.c_out}, ={self.})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.5628, grad_fn=<DivBackward0>),\n",
       " tensor(2.3645, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in = 10\n",
    "x = torch.rand(64, c_in).to(device=default_device())\n",
    "x = F.softmax(x, dim=1)\n",
    "label = x.max(dim=1).indices\n",
    "CenterLoss(c_in)(x, label), CenterPlusLoss(LabelSmoothingCrossEntropyFlat(), c_in)(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CenterPlusLoss(loss=FlattenedLoss of LabelSmoothingCrossEntropy(), c_out=10, =0.01)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CenterPlusLoss(LabelSmoothingCrossEntropyFlat(), c_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalLoss(Module):\n",
    "\n",
    "    def __init__(self, gamma=0, eps=1e-7):\n",
    "        self.gamma, self.eps, self.ce = gamma, eps, CrossEntropyLossFlat()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7478)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in = 10\n",
    "x = torch.rand(64, c_in).to(device=default_device())\n",
    "x = F.softmax(x, dim=1)\n",
    "label = x.max(dim=1).indices\n",
    "FocalLoss(c_in)(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 003_data.transforms.ipynb.\n",
      "Converted 003b_data.image.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 007_metrics.ipynb.\n",
      "Converted 008_learner.ipynb.\n",
      "Converted 009_optimizer.ipynb.\n",
      "Converted 010_callback.ipynb.\n",
      "Converted 100_models.utils.ipynb.\n",
      "Converted 100b_models.layers.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.FCN.ipynb.\n",
      "Converted 103b_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
