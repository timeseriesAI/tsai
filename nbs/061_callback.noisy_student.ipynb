{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.noisy_student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy student\n",
    "\n",
    "> Callback to apply noisy student self-training (a semi-supervised learning approach) based on: Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.data.preprocessing import *\n",
    "from tsai.data.transforms import *\n",
    "from tsai.models.layers import *\n",
    "from fastai.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# This is an unofficial implementation of noisy student based on:\n",
    "# Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. \n",
    "# In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n",
    "# Official tensorflow implementation available in https://github.com/google-research/noisystudent\n",
    "\n",
    "\n",
    "class NoisyStudent(Callback):\n",
    "    \"\"\"A callback to implement the Noisy Student approach. In the original paper this was used in combination with noise: \n",
    "        - stochastic depth: .8\n",
    "        - RandAugment: N=2, M=27\n",
    "        - dropout: .5\n",
    "        \n",
    "    Steps:\n",
    "        1. Build the dl you will use as a teacher\n",
    "        2. Create dl2 with the pseudolabels (either soft or hard preds)\n",
    "        3. Pass any required batch_tfms to the callback\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dl2:DataLoader, bs:Optional[int]=None, l2pl_ratio:int=1, batch_tfms:Optional[list]=None, do_setup:bool=True, \n",
    "                 pseudolabel_sample_weight:float=1., verbose=False): \n",
    "        r'''\n",
    "        Args:\n",
    "            dl2:                       dataloader with the pseudolabels\n",
    "            bs:                        batch size of the new, combined dataloader. If None, it will pick the bs from the labeled dataloader.\n",
    "            l2pl_ratio:                ratio between labels and pseudolabels in the combined batch\n",
    "            batch_tfms:                transforms applied to the combined batch. If None, it will pick the batch_tfms from the labeled dataloader (if any)\n",
    "            do_setup:                  perform a transform setup on the labeled dataset.\n",
    "            pseudolabel_sample_weight: weight of each pseudolabel sample relative to the labeled one of the loss.\n",
    "        '''\n",
    "        \n",
    "        self.dl2, self.bs, self.l2pl_ratio, self.batch_tfms, self.do_setup, self.verbose = dl2, bs, l2pl_ratio, batch_tfms, do_setup, verbose\n",
    "        self.pl_sw = pseudolabel_sample_weight\n",
    "        \n",
    "    def before_fit(self):\n",
    "        if self.batch_tfms is None: self.batch_tfms = self.dls.train.after_batch\n",
    "        self.old_bt = self.dls.train.after_batch # Remove and store dl.train.batch_tfms\n",
    "        self.old_bs = self.dls.train.bs\n",
    "        self.dls.train.after_batch = noop        \n",
    "\n",
    "        if self.do_setup and self.batch_tfms:\n",
    "            for bt in self.batch_tfms: \n",
    "                bt.setup(self.dls.train)\n",
    "\n",
    "        if self.bs is None: self.bs = self.dls.train.bs\n",
    "        self.dl2.bs = min(len(self.dl2.dataset), int(self.bs / (1 + self.l2pl_ratio)))\n",
    "        self.dls.train.bs = self.bs - self.dl2.bs\n",
    "        pv(f'labels / pseudolabels per training batch              : {self.dls.train.bs} / {self.dl2.bs}', self.verbose)\n",
    "        rel_weight = (self.dls.train.bs/self.dl2.bs) * (len(self.dl2.dataset)/len(self.dls.train.dataset))\n",
    "        pv(f'relative labeled/ pseudolabel sample weight in dataset: {rel_weight:.1f}', self.verbose)\n",
    "        self.dl2iter = iter(self.dl2)\n",
    "    \n",
    "        self.old_loss_func = self.learn.loss_func\n",
    "        self.learn.loss_func = self.loss\n",
    "        \n",
    "    def before_batch(self):\n",
    "        if self.training:\n",
    "            X, y = self.x, self.y\n",
    "            try: X2, y2 = next(self.dl2iter)\n",
    "            except StopIteration:\n",
    "                self.dl2iter = iter(self.dl2)\n",
    "                X2, y2 = next(self.dl2iter)\n",
    "            if y.ndim == 1 and y2.ndim == 2: y = torch.eye(self.learn.dls.c)[y].to(device) # ensure both \n",
    "            \n",
    "            X_comb, y_comb = concat(X, X2), concat(y, y2)\n",
    "            \n",
    "            if self.batch_tfms is not None: \n",
    "                X_comb = compose_tfms(X_comb, self.batch_tfms, split_idx=0)\n",
    "                y_comb = compose_tfms(y_comb, self.batch_tfms, split_idx=0)\n",
    "            self.learn.xb = (X_comb,)\n",
    "            self.learn.yb = (y_comb,)\n",
    "            pv(f'\\nX: {X.shape}  X2: {X2.shape}  X_comb: {X_comb.shape}', self.verbose)\n",
    "            pv(f'y: {y.shape}  y2: {y2.shape}  y_comb: {y_comb.shape}', self.verbose)\n",
    "            \n",
    "    def loss(self, output, target): \n",
    "        if target.ndim == 2: _, target = target.max(dim=1)\n",
    "        if self.training and self.pl_sw != 1: \n",
    "            loss = (1 - self.pl_sw) * self.old_loss_func(output[:self.dls.train.bs], target[:self.dls.train.bs])\n",
    "            loss += self.pl_sw * self.old_loss_func(output[self.dls.train.bs:], target[self.dls.train.bs:])\n",
    "            return loss \n",
    "        else: \n",
    "            return self.old_loss_func(output, target)\n",
    "    \n",
    "    def after_fit(self):\n",
    "        self.dls.train.after_batch = self.old_bt\n",
    "        self.learn.loss_func = self.old_loss_func\n",
    "        self.dls.train.bs = self.old_bs\n",
    "        self.dls.bs = self.old_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.models.all import *\n",
    "from tsai.tslearner import *\n",
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels / pseudolabels per training batch              : 171 / 85\n",
      "relative labeled/ pseudolabel sample weight in dataset: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.973474</td>\n",
       "      <td>1.819007</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: (171, 24, 51)  X2: (85, 24, 51)  X_comb: (256, 24, 43)\n",
      "y: (171,)  y2: torch.Size([85])  y_comb: (256,)\n"
     ]
    }
   ],
   "source": [
    "pseudolabeled_data = X\n",
    "soft_preds = True\n",
    "\n",
    "pseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\n",
    "dsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\n",
    "dl2 = TSDataLoader(dsets2, num_workers=0)\n",
    "noisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\n",
    "learn = TSClassifier(X, y, splits=splits, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels / pseudolabels per training batch              : 171 / 85\n",
      "relative labeled/ pseudolabel sample weight in dataset: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.798368</td>\n",
       "      <td>1.785091</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: (171, 24, 51)  X2: (85, 24, 51)  X_comb: (256, 24, 61)\n",
      "y: torch.Size([171, 6])  y2: torch.Size([85, 6])  y_comb: torch.Size([256, 6])\n"
     ]
    }
   ],
   "source": [
    "pseudolabeled_data = X\n",
    "soft_preds = False\n",
    "\n",
    "pseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\n",
    "dsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\n",
    "dl2 = TSDataLoader(dsets2, num_workers=0)\n",
    "noisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\n",
    "learn = TSClassifier(X, y, splits=splits, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
