{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.mixed_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label-mixing transforms\n",
    "\n",
    "> Callbacks that perform data augmentation by mixing samples in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.distributions.beta import Beta\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.layers import NoneReduce\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _reduce_loss(loss, reduction='mean'):\n",
    "    \"Reduce the loss based on `reduction`\"\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixHandler1d(Callback):\n",
    "    \"A handler class for implementing mixed sample data augmentation\"\n",
    "    run_valid = False\n",
    "\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.distrib = Beta(alpha, alpha)\n",
    "\n",
    "    def before_train(self):\n",
    "        self.labeled = self.dls.d\n",
    "        if self.labeled:\n",
    "            self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "            if self.stack_y: self.old_lf, self.learn.loss_func = self.learn.loss_func, self.lf\n",
    "\n",
    "    def after_train(self):\n",
    "        if self.labeled and self.stack_y: self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def lf(self, pred, *yb):\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf: loss = torch.lerp(lf(pred, *self.yb1), lf(pred, *yb), self.lam)\n",
    "        return _reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixUp1d(MixHandler1d):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412\"\n",
    "\n",
    "    def __init__(self, alpha=.4):\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.x.size(0), ))\n",
    "        self.lam = torch.max(lam, 1 - lam).to(self.x.device)\n",
    "        shuffle = torch.randperm(self.x.size(0))\n",
    "        xb1 = self.x[shuffle]\n",
    "        self.learn.xb = L(xb1, self.xb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.x.ndim - 1))\n",
    "        if self.labeled:\n",
    "            self.yb1 = tuple((self.y[shuffle], ))\n",
    "            if not self.stack_y: self.learn.yb = L(self.yb1, self.yb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.y.ndim - 1))    \n",
    "                \n",
    "MixUp1D = MixUp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.9250454902648926, 1.826296329498291, '00:06']\n"
     ]
    }
   ],
   "source": [
    "from fastai.learner import *\n",
    "from tsai.models.InceptionTime import *\n",
    "from tsai.data.external import get_UCR_data\n",
    "from tsai.data.core import get_ts_dls, TSCategorize\n",
    "from tsai.data.preprocessing import TSStandardize\n",
    "from tsai.learner import ts_learner\n",
    "\n",
    "X, y, splits = get_UCR_data('NATOPS', return_split=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, InceptionTime, cbs=MixUp1d(0.4))\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CutMix1d(MixHandler1d):\n",
    "    \"Implementation of `https://arxiv.org/abs/1905.04899`\"\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def before_batch(self):\n",
    "        bs, *_, seq_len = self.x.size()\n",
    "        self.lam = self.distrib.sample((1, ))\n",
    "        shuffle = torch.randperm(bs)\n",
    "        xb1 = self.x[shuffle]\n",
    "        x1, x2 = self.rand_bbox(seq_len, self.lam)\n",
    "        self.learn.xb[0][..., x1:x2] = xb1[..., x1:x2]\n",
    "        self.lam = (1 - (x2 - x1) / float(seq_len)).item()\n",
    "        if self.labeled:\n",
    "            self.yb1 = tuple((self.y[shuffle], ))\n",
    "            if not self.stack_y:\n",
    "                self.learn.yb = tuple(L(self.yb1, self.yb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.y.ndim - 1)))\n",
    "\n",
    "    def rand_bbox(self, seq_len, lam):\n",
    "        cut_seq_len = torch.round(seq_len * (1. - lam)).type(torch.long)\n",
    "        half_cut_seq_len = torch.div(cut_seq_len, 2, rounding_mode='floor')\n",
    "\n",
    "        # uniform\n",
    "        cx = torch.randint(0, seq_len, (1, ))\n",
    "        x1 = torch.clamp(cx - half_cut_seq_len, 0, seq_len)\n",
    "        x2 = torch.clamp(cx + half_cut_seq_len, 0, seq_len)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IntraClassCutMix1d(Callback):\n",
    "    \"Implementation of CutMix applied to examples of the same class\"\n",
    "    run_valid = False\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        bs, *_, seq_len = self.x.size()\n",
    "        idxs = torch.arange(bs, device=self.x.device)\n",
    "        y = torch.tensor(self.y)\n",
    "        unique_c = torch.unique(y).tolist()\n",
    "        idxs_by_class = torch.cat([idxs[torch.eq(y, c)] for c in unique_c])\n",
    "        idxs_shuffled_by_class = torch.cat([random_shuffle(idxs[torch.eq(y, c)]) for c in unique_c])\n",
    "        self.lam = self.distrib.sample((1, ))\n",
    "        x1, x2 = self.rand_bbox(seq_len, self.lam)\n",
    "        xb1 = self.x[idxs_shuffled_by_class]\n",
    "        self.learn.xb[0][idxs_by_class, :, x1:x2] = xb1[..., x1:x2]\n",
    "\n",
    "    def rand_bbox(self, seq_len, lam):\n",
    "        cut_seq_len = torch.round(seq_len * (1. - lam)).type(torch.long)\n",
    "        half_cut_seq_len = torch.div(cut_seq_len, 2, rounding_mode='floor')\n",
    "\n",
    "        # uniform\n",
    "        cx = torch.randint(0, seq_len, (1, ))\n",
    "        x1 = torch.clamp(cx - half_cut_seq_len, 0, seq_len)\n",
    "        x2 = torch.clamp(cx + half_cut_seq_len, 0, seq_len)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.781386375427246, 1.7941926717758179, '00:04']\n"
     ]
    }
   ],
   "source": [
    "X, y, splits = get_UCR_data('NATOPS', split_data=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, InceptionTime, cbs=IntraClassCutMix1d())\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.7089701890945435, 1.777895450592041, '00:05']\n"
     ]
    }
   ],
   "source": [
    "X, y, splits = get_UCR_data('NATOPS', split_data=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, cbs=CutMix1d(1.))\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.save_checkpoint();",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_export.ipynb.\n",
      "Converted 001_utils.ipynb.\n",
      "Converted 010_data.validation.ipynb.\n",
      "Converted 011_data.preparation.ipynb.\n",
      "Converted 012_data.external.ipynb.\n",
      "Converted 013_data.core.ipynb.\n",
      "Converted 014_data.unwindowed.ipynb.\n",
      "Converted 015_data.metadatasets.ipynb.\n",
      "Converted 016_data.preprocessing.ipynb.\n",
      "Converted 017_data.transforms.ipynb.\n",
      "Converted 018_data.mixed_augmentation.ipynb.\n",
      "Converted 019_data.image.ipynb.\n",
      "Converted 020_data.features.ipynb.\n",
      "Converted 021_data.tabular.ipynb.\n",
      "Converted 022_data.mixed.ipynb.\n",
      "Converted 050_losses.ipynb.\n",
      "Converted 051_metrics.ipynb.\n",
      "Converted 052_learner.ipynb.\n",
      "Converted 052_tsimage.learner.ipynb.\n",
      "Converted 052a_inference.ipynb.\n",
      "Converted 052b_analysis.ipynb.\n",
      "Converted 052c_calibration.ipynb.\n",
      "Converted 052d_tslearner.ipynb.\n",
      "Converted 053_optimizer.ipynb.\n",
      "Converted 060_callback.core.ipynb.\n",
      "Converted 060_callback.experimental.ipynb.\n",
      "Converted 061_callback.noisy_student.ipynb.\n",
      "Converted 063_callback.MVP.ipynb.\n",
      "Converted 064_callback.PredictionDynamics.ipynb.\n",
      "Converted 100_models.layers.ipynb.\n",
      "Converted 100b_models.utils.ipynb.\n",
      "Converted 100b_tabular.models.utils.ipynb.\n",
      "Converted 100b_tsimage.models.utils.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 111b_models.MINIROCKET.ipynb.\n",
      "Converted 111c_models.MINIROCKET_Pytorch.ipynb.\n",
      "Converted 111d_models.MINIROCKETPlus_Pytorch.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 114b_models.XCMPlus.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 121_models.TabTransformer.ipynb.\n",
      "Converted 122_models.TabFusionTransformer.ipynb.\n",
      "Converted 123_models.TSPerceiver.ipynb.\n",
      "Converted 124_models.TSiTPlus.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 140_models.misc.ipynb.\n",
      "Converted 200_optuna.ipynb.\n",
      "Converted 201_wandb.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/notebooks/tsai2/tsai\n",
      "basics.py                      saved      32833 s ago ***\n",
      "Incorrect conversion! ðŸ˜”\n",
      "Total time elapsed 34036.886 s\n",
      "Monday 28/02/22 10:49:58 CET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,UklGRvQHAABXQVZFZm10IBAAAAABAAEAECcAACBOAAACABAAZGF0YdAHAAAAADoKZBRsHkMo2DEbO/1DcExmVNFbpmLaaGJuNXNMd596KX3nftR/8X87f7Z9YntFeGJ0wW9pamNkuV10VqNOUEaLPWA04CoaIR0X+wzDAoj4WO5F5F/at9Bcx1++zLWzrSGmIp/CmAuTBo68iTOGcoN9gVeAAoB/gM2B6YPQhn6K7I4UlOyZa6CHpzSvZbcNwB3JiNI93CzmRvB6+rYE6g4HGfoitSwmNj4/70cqUOJXCV+WZXtrsXAvdex443sQfm5/+3+3f6J+vXwMepN2WHJibblnZmFzWuxS3UpUQl45CzBoJoccdxJJCA7+1fOw6bDf5dVfzC3DX7oCsiaq1qIfnAuWpZD1iwOI1oRygtyAFoAigP+ArIImhWmIcYw1ka+W1Zyeo/6q6bJTuy3EaM331sngzur29DD/agmWE6EdfScXMWE6TEPIS8hTP1shYmJo+G3acv92Ynr9fMt+yX/2f1F/3H2Ze4x4uXQncN1q5GRGXg5XR0/+RkE+HjWlK+Mh6hfKDZQDWPkn7xHlJ9t50RjIEr93tlSutqarnz6ZeZNmjgyKdIajg56BZ4ABgG6AqoG2g46GLIqLjqSTb5nhn/Gmkq65tli/YcjF0XXbYOV376n55QMbDjoYMiLxK2g1iD5CR4dPSVd9XhZlCmtOcNp0p3iue+t9Wn/4f8R/wH7rfEp64Xa2cs5tM2jtYQZbi1OGSwZDGTrMMC8nUh1FExkJ3v6l9H7qeuCq1h7N5cMOu6iywapmo6KcgZYMkU6MTIgPhZuC9YAegBmA5oCCguyEIIgXjM2QOZZSnA6jYqpDsqO6dMOpzDLW/98A6ib0X/6aCMcS1hy2JlYwpzmaQh9LKlOsWpph6GeMbX1ysnYles98rn68f/p/Zn8Cfs970XgOdYtwT2tkZdNeplfqT6xH9z7cNWksrCK3GJoOZAQp+vbv3eXv2zzS1MjGvyK39a5MpzWgu5nok8aOXoq2htWDv4F4gAKAXYCKgYWDTIbbiSuONpPymFifW6byrQ62pL6lxwPRrdqU5Kju2fgUA0wNbRdoIS0rqjTSPZRG406wVvBdlmSXaulvhHRheHh7xX1Ef/N/0H/cfhh9h3oudxJzOW6raHJimFsoVC9MuEPSOo0x9icdHhQU6Qmv/3X1TOtF4XDX3s2dxL67T7Ndq/ajJp33lnWRqIyXiEqFxoIPgSiAEoDOgFqCs4TXh7+LZpDElc+bf6LIqZ2x9Lm9wurLbdU13zPpVvOO/ckH+RELHO8llC/sOOdBdkqLUhlaE2FuZyBtH3JjduZ5oHyPfq5//X96fyZ+A3wWeWJ17nDAa+NlX18+WI1QWEitP5k2LC11I4QZaQ81Bfn6xfCp5rfc/9KRyXvAzreXr+OnwKA5mliUKI+xivmGCITigYuABIBOgGqBVYMMhouJzI3IkneYz57HpVKtZLXxverGQdDl2cjj2u0J+EQCfAygFp8gaCrsMxs95kU+ThZWYl0UZCJqg28tdBl4QHuefS1/7X/af/d+RH3Denp3bHOibiNp92IqXMVU1kxpRIw7TTK8KOge4hS5Cn8ARfYa7BDiNtiezlfFb7z3s/qriKSrnW+X35EDjeOIhoXygiqBMoAMgLeAMoJ8hJCHaYsBkFCVTpvxoS6p+bBFuQbCLMuo1GveZuiH8r78+QYqET8bJyXSLjE4NEHMSetRhFmKYPJmsmzAcRN2pXlwfG9+n3/+f4x/SH42fFl5tHVPcTFsYWbpX9VYL1EESWJAVjfvLT0kUBo4EAUGyvuU8XbngN3D007KMcF7uDqwe6hMobiaypSLjwWLPoc9hAaCnoAHgEGATIEng86FPYlvjVyS/ZdInjOls6y7tD+9MMZ/zx7Z/eIL7Tj3cwGsC9IV1B+jKS0zYzw2RZhNe1XSXJFjrWkcb9Vz0HcHe3V9FX/lf+R/EX9uff56xHfGcwtvmWl7Y7pcYVV8TRlFRDwNM4Epsh+wFYkLUAEV9+js2+L92F/PEMYhvZ+0mKwapTGe6JdKkl+NMInDhR+DR4E/gAeAooANgkaESocTi5yP3ZTOmmShlahVsJi4T8FuyuTTot2Z57fx7fspBlsQcxpfJBAudTeAQCFJSlHuWAFgdmZDbF9xwnVkeT98Tn6Pf/5/nX9qfmh8mnkGdrBxoGzeZnNga1nQUa9JFkESOLIuBSUcGwcR1gaa/GTyQ+hJ3ofUDMvnwSi53bAUqdmhOJs9lfCPWouEh3OELIKzgAuANIAvgfmCkIXwiBKN8ZGEl8KdoKQVrBO0jbx2xb7OWNgy4j3saPaiANwKBBUKH90obTKrO4ZE8kzgVEJcDmM3abRufHOGd816S337ftx/638pf5d9N3sNeB50cm8Pav5jSl38VSJOyEX8PMwzRyp9IH0WWQwgAuX3t+2m48TZINDLxtO9SLU3ra6luJ5imLaSvI1+iQKGTYNlgUyABICOgOiBEYQFh7+KOY9rlE6a2KD9p7Kv67eawLDJINPZ3Mzm6PAc+1gFjA+mGZcjTS25Nss/dUioUFhYdl/4ZdNr/nBwdSF5DHwsfn1//X+sf4p+mHzbeVZ2D3INbVln/GAAWnBSWUrJQc04dC/NJegb1hGmB2v9M/MQ6RPfS9XKy57C1rmCsa6pZ6K5m7CVVZCxi8uHqoRTgg==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from tsai.imports import create_scripts\n",
    "from tsai.export import get_nb_name\n",
    "nb_name = get_nb_name()\n",
    "create_scripts(nb_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
