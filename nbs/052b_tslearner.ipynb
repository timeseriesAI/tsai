{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tslearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSLearners (TSClassifier, TSRegressor, TSForecaster)\n",
    "\n",
    "> This contains a set of time series learners with a new API that simplifies the learner creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.learner import *\n",
    "from tsai.data.all import *\n",
    "from tsai.models.InceptionTime import *\n",
    "from tsai.models.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSClassifier API\n",
    "***\n",
    "\n",
    "**Commonly used arguments:**\n",
    "    \n",
    "* **X:** array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\n",
    "* **y:** array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. \n",
    "* **splits:** lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.\n",
    "* **tfms:** item transforms that will be applied to each sample individually. Default:`[None, TSClassification()]` which is commonly used in most single label datasets. \n",
    "* **batch_tfms:** transforms applied to each batch. Default=None. \n",
    "* **bs:** batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=`[64, 128]`. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). \n",
    "* **arch:** indicates which architecture will be used. Default: InceptionTime.\n",
    "* **arch_config:** keyword arguments passed to the selected architecture. Default={}.\n",
    "* **pretrained:** indicates if pretrained model weights will be used. Default=False.\n",
    "* **weights_path:** indicates the path to the pretrained weights in case they are used.\n",
    "* **loss_func:** allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\n",
    "* **opt_func:** allows you to pass an optimizer. Default=Adam.\n",
    "* **lr:** learning rate. Default=0.001.\n",
    "* **metrics:** list of metrics passed to the Learner. Default=accuracy.\n",
    "* **cbs:** list of callbacks passed to the Learner. Default=None.\n",
    "* **wd:** is the default weight decay used when training the model. Default=None.\n",
    "\n",
    "**Less frequently used arguments:**\n",
    "\n",
    "* **sel_vars:** used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.`[0,3,5]`).\n",
    "* **sel_steps:** used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. `slice(-50, None)` will select the last 50 steps from each time series).\n",
    "* **inplace:** indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training.\n",
    "* **shuffle_train:** indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.\n",
    "* **drop_last:** if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.\n",
    "* **num_workers:** num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0. \n",
    "* **do_setup:** ndicates if the Pipeline.setup method should be called during initialization. Default=True.\n",
    "* **device:** Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').\n",
    "* **verbose:** controls the verbosity when fitting and predicting.\n",
    "* **exclude_head:** indicates whether the head of the pretrained model needs to be removed or not. Default=True.\n",
    "* **cut:** indicates the position where the pretrained model head needs to be cut. Defaults=-1.\n",
    "* **init:** allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\n",
    "* **splitter:** To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\n",
    "* **path** and **model_dir:** are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\n",
    "* **wd_bn_bias:** controls if weight decay is applied to BatchNorm layers and bias. Default=False.\n",
    "train_bn=True\n",
    "* **moms:** the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class TSClassifier(Learner):\n",
    "    def __init__(self, X, y=None, splits=None, tfms=None, inplace=True, sel_vars=None, sel_steps=None,\n",
    "                 bs=[64, 128], batch_size=None, batch_tfms=None, shuffle_train=True, drop_last=True, num_workers=0, do_setup=True, device=None,\n",
    "                 arch=None, arch_config={}, pretrained=False, weights_path=None, exclude_head=True, cut=-1, init=None,\n",
    "                 loss_func=None, opt_func=Adam, lr=0.001, metrics=accuracy, cbs=None, wd=None, wd_bn_bias=False,\n",
    "                 train_bn=True, moms=(0.95, 0.85, 0.95),  path='.', model_dir='models', splitter=trainable_params, verbose=False):\n",
    "\n",
    "        #Splits\n",
    "        if splits is None: splits = TSSplitter()(X)\n",
    "            \n",
    "        # Item tfms\n",
    "        if tfms is None: tfms = [None, TSClassification()]\n",
    "\n",
    "        # Batch size\n",
    "        if batch_size is not None:\n",
    "            bs = batch_size\n",
    "\n",
    "        # DataLoaders\n",
    "        dls = get_ts_dls(X, y=y, splits=splits, sel_vars=sel_vars, sel_steps=sel_steps, tfms=tfms, inplace=inplace, path=path, bs=bs,\n",
    "                         batch_tfms=batch_tfms, num_workers=num_workers, device=device, shuffle_train=shuffle_train, drop_last=drop_last)\n",
    "        \n",
    "        if loss_func is None:\n",
    "            if hasattr(dls, 'loss_func'): loss_func = dls.loss_func\n",
    "            elif hasattr(dls, 'cat') and not dls.cat: loss_func = MSELossFlat()\n",
    "            elif hasattr(dls, 'train_ds') and hasattr(dls.train_ds, 'loss_func'): loss_func = dls.train_ds.loss_func\n",
    "            else: loss_func = CrossEntropyLossFlat()\n",
    "        \n",
    "        # Model\n",
    "        if init is True:\n",
    "            init = nn.init.kaiming_normal_\n",
    "        if arch is None:\n",
    "            arch = InceptionTime\n",
    "        if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n",
    "            model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, **arch_config)\n",
    "        elif 'tabularmodel' in arch.__name__.lower():\n",
    "            build_tabular_model(arch, dls=dls, device=device, **arch_config)\n",
    "        else:\n",
    "            model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n",
    "                                   exclude_head=exclude_head, cut=cut, init=init, **arch_config)\n",
    "        setattr(model, \"__name__\", arch.__name__)\n",
    "        try:\n",
    "            model[0], model[1]\n",
    "            splitter = ts_splitter\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        super().__init__(dls, model, loss_func=loss_func, opt_func=opt_func, lr=lr, cbs=cbs, metrics=metrics, path=path, splitter=splitter,\n",
    "                         model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.613247</td>\n",
       "      <td>1.415379</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacho/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "from tsai.models.InceptionTimePlus import *\n",
    "X, y, splits = get_classification_data('OliveOil', split_data=False)\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "learn = TSClassifier(X, y, splits=splits, batch_tfms=batch_tfms, metrics=accuracy, arch=InceptionTimePlus, arch_config=dict(fc_dropout=.5))\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSRegressor API\n",
    "***\n",
    "\n",
    "**Commonly used arguments:**\n",
    "    \n",
    "* **X:** array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\n",
    "* **y:** array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. \n",
    "* **splits:** lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.\n",
    "* **tfms:** item transforms that will be applied to each sample individually. Default=`[None, TSRegression()]` which is commonly used in most single label datasets. \n",
    "* **batch_tfms:** transforms applied to each batch. Default=None. \n",
    "* **bs:** batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=`[64, 128]`. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). \n",
    "* **arch:** indicates which architecture will be used. Default: InceptionTime.\n",
    "* **arch_config:** keyword arguments passed to the selected architecture. Default={}.\n",
    "* **pretrained:** indicates if pretrained model weights will be used. Default=False.\n",
    "* **weights_path:** indicates the path to the pretrained weights in case they are used.\n",
    "* **loss_func:** allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\n",
    "* **opt_func:** allows you to pass an optimizer. Default=Adam.\n",
    "* **lr:** learning rate. Default=0.001.\n",
    "* **metrics:** list of metrics passed to the Learner. Default=None.\n",
    "* **cbs:** list of callbacks passed to the Learner. Default=None.\n",
    "* **wd:** is the default weight decay used when training the model. Default=None.\n",
    "\n",
    "**Less frequently used arguments:**\n",
    "\n",
    "* **sel_vars:** used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.`[0,3,5]`).\n",
    "* **sel_steps:** used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. `slice(-50, None)` will select the last 50 steps from each time series).\n",
    "* **inplace:** indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training.\n",
    "* **shuffle_train:** indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.\n",
    "* **drop_last:** if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.\n",
    "* **num_workers:** num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0. \n",
    "* **do_setup:** ndicates if the Pipeline.setup method should be called during initialization. Default=True.\n",
    "* **device:** Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').\n",
    "* **verbose:** controls the verbosity when fitting and predicting.\n",
    "* **exclude_head:** indicates whether the head of the pretrained model needs to be removed or not. Default=True.\n",
    "* **cut:** indicates the position where the pretrained model head needs to be cut. Defaults=-1.\n",
    "* **init:** allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\n",
    "* **splitter:** To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\n",
    "* **path** and **model_dir:** are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\n",
    "* **wd_bn_bias:** controls if weight decay is applied to BatchNorm layers and bias. Default=False.\n",
    "train_bn=True\n",
    "* **moms:** the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    " \n",
    "        \n",
    "class TSRegressor(Learner):\n",
    "    def __init__(self, X, y=None, splits=None, tfms=None, inplace=True, sel_vars=None, sel_steps=None, \n",
    "                 bs=[64, 128], batch_size=None, batch_tfms=None, shuffle_train=True, drop_last=True, num_workers=0, do_setup=True, device=None,\n",
    "                 arch=None, arch_config={}, pretrained=False, weights_path=None, exclude_head=True, cut=-1, init=None,\n",
    "                 loss_func=None, opt_func=Adam, lr=0.001, metrics=None, cbs=None, wd=None, wd_bn_bias=False,\n",
    "                 train_bn=True, moms=(0.95, 0.85, 0.95),  path='.', model_dir='models', splitter=trainable_params, verbose=False):\n",
    "\n",
    "        #Splits\n",
    "        if splits is None: splits = TSSplitter()(X)\n",
    "            \n",
    "        # Item tfms\n",
    "        if tfms is None: tfms = [None, TSRegression()]\n",
    "\n",
    "        # Batch size\n",
    "        if batch_size is not None:\n",
    "            bs = batch_size\n",
    "\n",
    "        # DataLoaders\n",
    "        dls = get_ts_dls(X, y=y, splits=splits, sel_vars=sel_vars, sel_steps=sel_steps, tfms=tfms, inplace=inplace, path=path, bs=bs,\n",
    "                         batch_tfms=batch_tfms, num_workers=num_workers, device=device, \n",
    "                         shuffle_train=shuffle_train, drop_last=drop_last)\n",
    "\n",
    "        if loss_func is None:\n",
    "            if hasattr(dls, 'loss_func'): loss_func = dls.loss_func\n",
    "            elif hasattr(dls, 'cat') and not dls.cat: loss_func = MSELossFlat()\n",
    "            elif hasattr(dls, 'train_ds') and hasattr(dls.train_ds, 'loss_func'): loss_func = dls.train_ds.loss_func\n",
    "            else: loss_func = MSELossFlat()\n",
    "                \n",
    "        # Model\n",
    "        if init is True:\n",
    "            init = nn.init.kaiming_normal_\n",
    "        if arch is None:\n",
    "            arch = InceptionTime\n",
    "        if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n",
    "            model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, **arch_config)\n",
    "        elif 'tabularmodel' in arch.__name__.lower():\n",
    "            build_tabular_model(arch, dls=dls, device=device, **arch_config)\n",
    "        else:\n",
    "            model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n",
    "                               exclude_head=exclude_head, cut=cut, init=init, **arch_config)\n",
    "        setattr(model, \"__name__\", arch.__name__)\n",
    "        try:\n",
    "            model[0], model[1]\n",
    "            splitter = ts_splitter\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        super().__init__(dls, model, loss_func=loss_func, opt_func=opt_func, lr=lr, cbs=cbs, metrics=metrics, path=path, splitter=splitter,\n",
    "                         model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>207.246979</td>\n",
       "      <td>192.114578</td>\n",
       "      <td>13.426781</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsai.models.TST import *\n",
    "X, y, splits = get_regression_data('AppliancesEnergy', split_data=False)\n",
    "batch_tfms = [TSStandardize()]\n",
    "learn = TSRegressor(X, y, splits=splits, batch_tfms=batch_tfms, arch=TST, metrics=mae, bs=512)\n",
    "learn.fit_one_cycle(1, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSForecaster API\n",
    "***\n",
    "**Commonly used arguments:**\n",
    "    \n",
    "* **X:** array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\n",
    "* **y:** array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. \n",
    "* **splits:** lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.\n",
    "* **tfms:** item transforms that will be applied to each sample individually. Default=`[None, TSForecasting()]` which is commonly used in most single label datasets. \n",
    "* **batch_tfms:** transforms applied to each batch. Default=None. \n",
    "* **bs:** batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=`[64, 128]`. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). \n",
    "* **arch:** indicates which architecture will be used. Default: InceptionTime.\n",
    "* **arch_config:** keyword arguments passed to the selected architecture. Default={}.\n",
    "* **pretrained:** indicates if pretrained model weights will be used. Default=False.\n",
    "* **weights_path:** indicates the path to the pretrained weights in case they are used.\n",
    "* **loss_func:** allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\n",
    "* **opt_func:** allows you to pass an optimizer. Default=Adam.\n",
    "* **lr:** learning rate. Default=0.001.\n",
    "* **metrics:** list of metrics passed to the Learner. Default=None.\n",
    "* **cbs:** list of callbacks passed to the Learner. Default=None.\n",
    "* **wd:** is the default weight decay used when training the model. Default=None.\n",
    "\n",
    "**Less frequently used arguments:**\n",
    "\n",
    "* **sel_vars:** used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.`[0,3,5]`).\n",
    "* **sel_steps:** used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. `slice(-50, None)` will select the last 50 steps from each time series).\n",
    "* **inplace:** indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training.\n",
    "* **shuffle_train:** indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.\n",
    "* **drop_last:** if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.\n",
    "* **num_workers:** num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=None. \n",
    "* **do_setup:** ndicates if the Pipeline.setup method should be called during initialization. Default=True.\n",
    "* **device:** Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').\n",
    "* **verbose:** controls the verbosity when fitting and predicting.\n",
    "* **exclude_head:** indicates whether the head of the pretrained model needs to be removed or not. Default=True.\n",
    "* **cut:** indicates the position where the pretrained model head needs to be cut. Defaults=-1.\n",
    "* **init:** allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\n",
    "* **splitter:** To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\n",
    "* **path** and **model_dir:** are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\n",
    "* **wd_bn_bias:** controls if weight decay is applied to BatchNorm layers and bias. Default=False.\n",
    "train_bn=True\n",
    "* **moms:** the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export  \n",
    "        \n",
    "class TSForecaster(Learner):\n",
    "    def __init__(self, X, y=None, splits=None, tfms=None, inplace=True, sel_vars=None, sel_steps=None, \n",
    "                 bs=[64, 128], batch_size=None, batch_tfms=None, shuffle_train=True, drop_last=True, num_workers=0, do_setup=True, device=None,\n",
    "                 arch=None, arch_config={}, pretrained=False, weights_path=None, exclude_head=True, cut=-1, init=None,\n",
    "                 loss_func=None, opt_func=Adam, lr=0.001, metrics=None, cbs=None, wd=None, wd_bn_bias=False,\n",
    "                 train_bn=True, moms=(0.95, 0.85, 0.95),  path='.', model_dir='models', splitter=trainable_params, verbose=False):\n",
    "\n",
    "        #Splits\n",
    "        if splits is None: splits = TSSplitter()(X)\n",
    "            \n",
    "        # Item tfms\n",
    "        if tfms is None: tfms = [None, TSForecasting()]\n",
    "\n",
    "        # Batch size\n",
    "        if batch_size is not None:\n",
    "            bs = batch_size\n",
    "\n",
    "        # DataLoaders\n",
    "        dls = get_ts_dls(X, y=y, splits=splits, sel_vars=sel_vars, sel_steps=sel_steps, tfms=tfms, inplace=inplace, path=path, bs=bs,\n",
    "                         batch_tfms=batch_tfms, num_workers=num_workers, device=device, \n",
    "                         shuffle_train=shuffle_train, drop_last=drop_last)\n",
    "        \n",
    "        if loss_func is None:\n",
    "            if hasattr(dls, 'loss_func'): loss_func = dls.loss_func\n",
    "            elif hasattr(dls, 'cat') and not dls.cat: loss_func = MSELossFlat()\n",
    "            elif hasattr(dls, 'train_ds') and hasattr(dls.train_ds, 'loss_func'): loss_func = dls.train_ds.loss_func\n",
    "            else: loss_func = MSELossFlat()\n",
    "        \n",
    "        # Model\n",
    "        if init is True:\n",
    "            init = nn.init.kaiming_normal_\n",
    "        if arch is None:\n",
    "            arch = InceptionTime\n",
    "        if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n",
    "            model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, **arch_config)\n",
    "        elif 'tabularmodel' in arch.__name__.lower():\n",
    "            build_tabular_model(arch, dls=dls, device=device, **arch_config)\n",
    "        else:\n",
    "            model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n",
    "                               exclude_head=exclude_head, cut=cut, init=init, **arch_config)\n",
    "        setattr(model, \"__name__\", arch.__name__)\n",
    "        try:\n",
    "            model[0], model[1]\n",
    "            splitter = ts_splitter\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        super().__init__(dls, model, loss_func=loss_func, opt_func=opt_func, lr=lr, cbs=cbs, metrics=metrics, path=path, splitter=splitter,\n",
    "                         model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Sunspots\n",
      "downloading data...\n",
      "...data downloaded. Path = data/forecasting/Sunspots.csv\n"
     ]
    }
   ],
   "source": [
    "from tsai.models.TSTPlus import *\n",
    "ts = get_forecasting_time_series('Sunspots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Sunspots\n",
      "downloading data...\n",
      "...data downloaded. Path = data/forecasting/Sunspots.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBwAAABTCAYAAAA82hSvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAT6UlEQVR4nO3de3SW1Z3o8W8iJgENARRnqOikUJGx2kLDQo84tY5bl057LGqPehxbXKcdi8ue03ZaKTOOR7zUhWtq9Yx6Sh3PWI/tWBmm2gurY/uboq0cQQgXRU0rMpSiyDUSrgGT9/zxvGljeHMleRPI97MWK3meZz9778hvbXl/2ZeSXC6HJEmSJElSbyrt7w5IkiRJkqSjjwkHSZIkSZLU60w4SJIkSZKkXmfCQZIkSZIk9ToTDpIkSZIkqdeZcJAkSZIkSb1uSH93QJKkQiKiFJgJ3AiMA3YAi4A7U0qvd7GOOcD0lNKkiLgeuD+lNKKH/bkemJNSqo6Ij+X7MjKl9E5P6mtTdw64PKX0dESsz7fzncOtN1/3erKf+/7eqE+SJKmrnOEgSRqo/hq4FbgbOAf4K+BPgKURMaYH9f0rMLnlIiJyETG9h31bCpwGNHRWMCK+ExFPd1LsNODnPexLZ+2dDzzaG3VLkiR1hzMcJEkD1Y3AHSmlJ/LXL0fEc8CbwBXAQ92pLKW0C9jVGx1LKe0D1h5uPRFxbErpYErpsOtqT0rpt31VtyRJUkdMOEiSBqqRwMmtb6SU9kXEXwDbIPttPlAO/A74HLCHLBFxT0op1/rd1ksq8ksYAJ6KiNtTSnPaNh4RFwL/AIwHVgD/3urZx2i1pCIiriWbjXEqsA64K6X0ZL5/M/LvrM8vx3gWqAVOAS4ETmi9pCLfxPsjIoBzgTeAW1JKP2qph1ZLJCKiGvgPstkbXyrQ3u/LR8QxwG3AZ4Hh+X58NaW0PP/Os2SzN0YBVwMHgPtSSl9v+99HkiSpMy6pkCQNVE8CfxsRv4iIWRFxTn5GwJI2MwKuACrJlg78DdkH/891Uvdp+a83kiUV3iMiTgJ+CDwPTAMeB75aqKKIOAP4v8DXgbOBh4HvR8QUYBbwAyDy/WtxE7AG+Gg7/ZsF/Bg4D3iGLDFyZic/U8t7hdpr8XdkP/OX8j/XC8BzEfEnrcp8EdgM/BnwLeCuiPjTLrQtSZL0Hs5wkCQNVP8deBX4L8CdQBmwMyL+Cbg5pdSUL7cF+EL++uWI+AjZB/p/bK/ilNLabAIBb6eUdhQocgPZrImZ+ZkStRHxIeDSAmVbkhcvpJTeANZExNvAzpTSlojYBRzTZmnD4pTSHR387I+klP5X/vsVEXEx2R4WX+zgHTpoj4goJ0ua/I+U0r/kb78UERcAXwBuzt+rTSn9Xf6dV4CvAWcBr3XUtiRJUlsmHCRJA1JK6V3gAeCBiBgK/Ceyaf5fAraTzSiA7ANyU6tXXyRLGByOScCSNssyXqRwwiGAZ4FXIuJ54BfAv3ZyksaaTtpf2ub6RbKTOg7HOOB4sr62tgo4vdX1ipZvUkrvRsReYNhhti1JkgYhl1RIkgaciDg3Ih5vuU4p7Usp/SKl9Hmy5Q0XtSp+oM3rQ4Hmw+zCsQXqOKZQwZTSnpRSIltO8QxwAfBqJydg5Dp4Vuj5EGBvO2UrOqmrxdD814Nt7g9rU3cTkiRJvcCEgyRpIGoErssvY2jrALCv1XXbMucCrxxm+6+RzahoW+8hIuK/RsStKaXVKaW/TyldBPyUbG+Jnjq7zfU5wEv57w/wh+QBwMQu1rmWLJnw+58rIkrI9mpY3bNuSpIktc8lFZKkASelVBsRPyXbLPEWsg/bf0R2qsMMsn0dWpwWEfcCj5FtwjiD7BSGzhwAPhQRvyywj8NDwBcj4kHgUbIP/JcDOwvUsx14NCK2AovJ9nQ4h+w0CMiSJ6dExPj8Hg9dcX1ErARWAteTndbxcP7ZGuAT+b4dR7ZRZmsF20spNUTEI8A3I+IA2fGinyc7keLbXeyXJElSlznDQZI0UF0OPEJ2ssIy4J+BM4GLUko/bFXu38iOeFxMtsHhzSmlx+ncPwF/S4ETLVJKvwM+CaR8vZ8EZheqJKX0s/yzr5LttfBNsoTFt/JF5gPVwI+60KcWd5BtErkE+HPgEymlrflns8iWd2wm2y/i/jbvdtTeXwMLyZIovwL+FLi4nY0zJUmSDktJLtfZMlJJkgamiPgOMCKlNL2fuyJJkqQ2nOEgSZIkSZJ6nQkHSZIkSZLU61xSIUmSJEmSep0zHCRJkiRJUq8r2rGY99xzz2jgYmA9sL9Y7UqSJEmSiqKC7KSkn33ta1/b2klZDQJFSziQJRu+W8T2JEmSJEnFdx3wvf7uhPpfMRMO6wG+//1PsHXrCUVsVpIkSZL6zsTvXtffXRgQKvdUcs6acyD/2U8qZsJhP8DWrSfw5pt/XMRmJUmSJKnv/NHw+v7uwkDjEnoBbhopSZIkSZL6gAkHSZIkSZLU64q5pEKSJEmSpCNGbW1tOTC8v/sxQDXU1NQ0dlTAhIMkSZIkSW185Stf+cuVK1fetGvXrmH93ZeBqLKycu/kyZMfuvfee9s9kcSEgyRJkiRJrdTW1pavXLnypg0bNpQB7/Z3fwai+vr6spKSkptqa2sXtDfTwT0cJEmSJEl6r+HObOhcQ0PDMDpYctLlhENEPBYRn+uVXkmSJEmSpKNap0sqIuJi4BLgOuBXfd4jSZIkSZJ0xOvKHg7TyKZIbOnjvkiSJEmSNGC98cbamr5uY/z4D9T2dRvF0mnCIaV0G0BEPNvnvZEkSZIkSV0ye/bs06urq48v9GzFihXbH3744fVdqefMM8+s/MxnPvP+WbNmvdSb/euTUyoiYg5wW+t7EydOpK6uri+akyRJkiRp0Jk7d+6vW76fPXv26WvXrt21YMGCt9qWKy0tpbm5ud161qxZs6u3kw3QRwmHlNIcYE7re/fcc08NsLwv2pMkSZIkSX9wwQUXnDB16tQTGhoaDo4dO/a4W265Zc1555036tJLL31fVVVVWX19fePChQvfXLJkyTutZzh86lOfet/o0aPLS0tLSyZOnFi1e/fudx977LF1dXV1e7rbB4/FlCRJkiTpKFRdXV35+uuv75ozZ84rZWVlJddcc031E088sf7LX/7yymeffXbz1VdfXV3ovbPOOmvU8uXLd9x8882r161b13DZZZed3JP2TThIkiRJknQU2rZt2/6I2Hbw4MFcLpfjvvvuq1uzZs3uYcOGHdPU1JQbOnToMaWlh6YF1q1b17B06dJ39u/f37xq1ap3RowYUdaT9vtkSYUkSZIkSepf+/bte7fl++bmZs4///zRN9xww/Bdu3Yd3LZtW2N77+3Zs+f37x04cKC5tLS0pCftdznhkFL6WE8akCRJkiRJ/ev8888fdeqppx536623rjlw4EBu3LhxQydNmjSqL9t0hoMkSZIkSUe5IUOGlJaUlFBeXl46YsSIY6ZPn34yQEVFRZ9ttWDCQZIkSZKkLhg//gO1/d2Hnnruuee2n3HGGVV33333h3bs2NH41FNPbRw2bNiQG2+8cfwzzzzzdl+0acJBkiRJkqQj3Ny5c3/d+nrRokXbFy1atL3lurGxsfn+++9f27rMqlWrGlq+nzVr1ksACxYseKt1mdWrV+9avXr1yz3pk6dUSJIkSZKkXmfCQZIkSZIk9ToTDpIkSZIkqdeZcJAkSZIkSb2umJtGVgCMHr29s3KSJEmSdMQY2TCyv7swIFTuqWz5tqI/+6GBo5gJh2qAa675SRGblCRJkqQ+tvSi/u7BQFMNLO7vTqj/FTPh8LPq6mrWr19/HrC/iO3qCDZx4sTldXV1U/q7HzoyGC/qLmNG3WXMqDuMF3XXURAzFWTJhp/1cz80QJTkcrmiNRYRuZRSSdEa1BHPmFF3GC/qLmNG3WXMqDuMF3WXMTNw1NbWjp45c+bP6+vr3219v2p+VU1ft73zqp21fd1Gbxk5cuSQefPmXVRTU7O10HM3jZQkSZIk6Qh0xRVXjLnzzjs/2Pb+tGnTRj7wwAOThw0b1u5n/tmzZ59+4YUXnggwb968mjFjxpQXKjd37tyzzjzzzMpCzzpjwkGSJEmSpCPQ4sWLt5944okV1dXVQ1vf/8hHPjLqtddee2fv3r3NXaln5syZtZs2bWrs7f4Vcw8HSZIkSZLUSzZv3nxgw4YNu6dOnTpy/fr1+wAqKipKJ0yYMPzRRx9946STTiqbMWNG9SmnnHLcwYMHm19++eX6xx9//HdNTU3v2Vth3rx5NbfffvuaTZs2NU6bNm3kZZddNnbo0KFDVqxYsb2kpOerfIo9w+H2IrenI58xo+4wXtRdxoy6y5hRdxgv6i5jRt22bNmy7R/+8IdHtVxPmTKlav/+/U2rVq1quPLKK0/etm3b/lmzZq3+xje+UXfGGWeMmDJlSlV7dY0ePfrYq6++unrBggUbZs+evbqxsbGpqqqqrKd9K2rCIaU0p5jt6chnzKg7jBd1lzGj7jJm1B3Gi7rLmFFPvPDCC/VVVVVl48aNGwowefLkUatXr97R3NzMwoULN82fP//NXC5HeXl56cGDB5uPP/74dlc6nHvuuSf85je/2bls2bKde/fubZ4/f/5bjY2NXVqWUYhLKiRJkiRJOkLt2bOnqa6u7p2pU6eOeuuttzZNmDBh+H333VcHMHbs2KE33HDD+Fwux+bNm/eVlpZ2uD5i1KhRZTt27DjQct3U1JTbvXv3wZ72zYSDJEmSJElHsKVLl26//PLLT9m4cePeHTt2NK5bt25fWVlZybXXXvv+Bx988Nd1dXV7AObMmXNGR/U0NDQcHDNmzO83oDz22GNLKisrj+1pvzylQpIkSZKkI9iKFSsaysvLj7nkkktOXr58+XaA0tLSktLS0pLy8vLSioqK0ksvvfSk0aNHV5SVlbWbB1i6dGn9hAkTqqZMmVI1bNiw0quuuurkIUOG9Dhv4AwHSZIkSZK6YOdVO2v7uw+FNDU15V566aUdZ5999kmLFy/eAbB///7mp59+esOMGTPG5XI5XnzxxW0LFy7c+PGPf3zssmXL6gvVs3Hjxv1PPvnk+iuvvPLUT3/600OWLFmyZcuWLft62q+SXC7XeSlJkiRJkgaJ2tra0TNnzvx5fX39u/3dl4Fs5MiRQ+bNm3dRTU3N1kLPizLDISI+CvxvYDxQC3w2pfTrYrStgSsifgT851a3tqeUToyIK4BvAH8MLCKLl7fz79wI3ApUAj8EZqaUdhe35yq2iHgM+FVK6ZH8dbtjivGjAvFScKzJPzNeBrGIuITs73888FvgjpTSPzvGqD0dxIzjjA4RETPIjrk8CXgF+HJK6XnHGA0mfb6HQ0QMB34A3A+8D3gOmN/X7eqI8AFgXEqpJP/nxIh4P/AY8EXgVGAr8G2AiDgPuAu4EjgNGA18vV96rqKIiIsj4pvAda3utTumGD+DW6F4yTtkrMmXN14GsYg4AVgA/APZh4G/Ab4TER/CMUYFdBIzjjN6j4g4HXgI+AJZvMwHfuC/YzTYFGPTyMuAdSmlR1JK9WRZvtMi4oNFaFsDVESUkA2yv23z6C+Bn6aUfpxS2kaWxf2LiBgBXA/8n5TSC/lM713AtcXrtfrBNGA4sKXVvY7GFONncDskXjoYa8B4Gew+CqxPKT2cUtqVUnoKeBmYjmOMCmsvZi7CcUaHuhD4ZUrpJ/lZCPPIkgTTcYzRIFKMhMMkYEXLRUrpAPAbssycBq+xQA54PiJ2R8SyiJjGofHyO2APUN32Gdn/5E/MD8I6CqWUbkspfQ5ovQRrEu2PKW2fGT+DSDvx0t5YA8bLYPc8cFXLRUScCIwDZuAYo8Lai5ktOM7oUN8GPhkRJRFRCdwIvAFMxjFGg0gx9nCoAra3ubeLbO2RBq/RwGvAV8kGzM8CPwFWAe+0KdsSL1Vtnu3Kf60s8I6OXh2NKW1jpKNnxs/gUHCsiYiJGC+DWkppK9l0ZSLiz4B/BF4ENtD1uOjomTFzlOkgZl7FcUZtpJSagKaISMDP87dvB07Gf8ccKRoqKyv31tfXl/V3Rway4cOH7wUa2ntejIRDPTCszb3j8vc1SKWUVgDntrp1f0T8FXAe8OM2xVvipW0sHZf/aiwNLh2NKd15ZvwMAp2MNcbLIJdfS/0g2RTnO8jWVN+NY4zaUShmUkrv4jijdqSUIiIqgLPJ9m7YAfxbm2KOMQNQTU1N4+TJkx8qKSm5qaGhoe3fi8iSDZMmTXqopqamsb0yxUg4vAr8t5aLiCgj21hnZRHa1gAVEZcCFfn1jy2GAMvIpoy1lBsDHAu8ThZLk8gGa4APAmvdnXfQ6WhMmYLxo1Y6GGt28YeYaClrvAwiETEU+BXwJnB6SmlT/r5jjArqIGYcZ3SIiJgN1KeUvp1SagR+GRH/D3iWLGHVUs4xZgC79957v1dbW7uAbI8oHaqho2QDFCfh8BRZpvdTZNOJbgOWpJTeLELbGrgqgG9FxCayY4I+m7/3eWBxfqriq2S/bXoipdSYP+ruiYh4kiybOxd4tD86r37V7pgSEd8Dao0ftdLeWPNL4D8wXgaza4FyYHp+DXULxxi1p72YcZxRIZuB2yLiRaCObNPR88g2gvyfjjFHjvwH6q393Y8jVZ9vGplS2kl2fMtdwNvAWWS7rGoQy/8W4O+BfwE2kp08cElK6WWypMN3ydbRlpCtiSSltAi4B/h3soH7lXwdGkQ6GlNSSq9j/KiVDsaa/cbLoDcZOB1ojIhcyx/gchxjVFh7MVOF44wO9RjwOPBDYBtZguD6lNJqHGM0iJTkcrn+7oMkSZIkSTrKFONYTEmSJEmSNMiYcJAkSZIkSb3OhIMkSZIkSep1JhwkSZIkSVKvM+EgSZIkSZJ6nQkHSZIkSZLU60w4SJIkSZKkXmfCQZIkSZIk9ToTDpIkSZIkqdf9f2g8wKXYu3ZzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x36 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10794.725586</td>\n",
       "      <td>6287.113770</td>\n",
       "      <td>58.876122</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsai.models.TSTPlus import *\n",
    "ts = get_forecasting_time_series('Sunspots')\n",
    "X, y = SlidingWindowSplitter(60, horizon=1)(ts)\n",
    "splits = TSSplitter(235)(y)\n",
    "batch_tfms = [TSStandardize(by_var=True)]\n",
    "learn = TSForecaster(X, y, splits=splits, batch_tfms=batch_tfms, arch=TST, arch_config=dict(fc_dropout=.5), metrics=mae, bs=512)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 000c_data.preparation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 002b_data.unwindowed.ipynb.\n",
      "Converted 002c_data.metadatasets.ipynb.\n",
      "Converted 003_data.preprocessing.ipynb.\n",
      "Converted 003b_data.transforms.ipynb.\n",
      "Converted 003c_data.mixed_augmentation.ipynb.\n",
      "Converted 003d_data.image.ipynb.\n",
      "Converted 003e_data.features.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 006_data.mixed.ipynb.\n",
      "Converted 050_losses.ipynb.\n",
      "Converted 051_metrics.ipynb.\n",
      "Converted 052_learner.ipynb.\n",
      "Converted 052b_tslearner.ipynb.\n",
      "Converted 053_optimizer.ipynb.\n",
      "Converted 060_callback.core.ipynb.\n",
      "Converted 061_callback.noisy_student.ipynb.\n",
      "Converted 063_callback.MVP.ipynb.\n",
      "Converted 064_callback.PredictionDynamics.ipynb.\n",
      "Converted 100_models.layers.ipynb.\n",
      "Converted 100b_models.utils.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 111b_models.MINIROCKET.ipynb.\n",
      "Converted 111c_models.MINIROCKET_Pytorch.ipynb.\n",
      "Converted 111d_models.MINIROCKETPlus_Pytorch.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 114b_models.XCMPlus.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 121_models.TabTransformer.ipynb.\n",
      "Converted 122_models.TabFusionTransformer.ipynb.\n",
      "Converted 123_models.TSPerceiver.ipynb.\n",
      "Converted 124_models.TSiTPlus.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 140_models.misc.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/Documents/Machine_Learning/Jupyter_Notebooks/tsai/tsai\n",
      "Correct conversion! ðŸ˜ƒ\n",
      "Total time elapsed 280 s\n",
      "Friday 10/09/21 19:35:55 CEST\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
