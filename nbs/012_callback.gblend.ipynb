{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.gblend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Blending\n",
    "\n",
    "> Callback used to apply gradient blending to multi-modal models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unofficial PyTorch implementation by Ignacio Oguiza of  - oguiza@gmail.com based on: Wang, W., Tran, D., & Feiszli, M. (2020). **What Makes Training Multi-Modal Classification Networks Hard?**. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12695-12705)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.all import *\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.data.preprocessing import *\n",
    "from tsai.data.transforms import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GBlendLoss(Module):\n",
    "    \"Wrapper loss used by the gradient blending callback to allow weights applied to each modality.\"\n",
    "\n",
    "    def __init__(self, crit=None, w=None):\n",
    "        self.crit = ifnone(crit, CrossEntropyLossFlat())\n",
    "        self.w = w\n",
    "        \n",
    "    def forward(self, preds, target):\n",
    "        # unweighted loss\n",
    "        if not is_listy(preds): return self.crit(preds, target)\n",
    "        \n",
    "        # weighted loss\n",
    "        if self.w is None: self.w = tensor([1.] * len(preds))\n",
    "        loss = 0\n",
    "        for i, pred in enumerate(preds): loss += self.crit(pred, target) * self.w[i]\n",
    "        return loss / sum(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GBlend(Callback):\n",
    "    r\"\"\"A callback to implement multi-modal gradient blending.\n",
    "    \n",
    "    This is an unofficial PyTorch implementation by Ignacio Oguiza of  - oguiza@gmail.com based on: Wang, W., Tran, D., & Feiszli, M. (2020). \n",
    "    What Makes Training Multi-Modal Classification Networks Hard?. In Proceedings of the IEEE/CVF Conference on Computer Vision and \n",
    "    Pattern Recognition (pp. 12695-12705).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, V_pct=.1, n:Union[None, int, tuple, list]=(10, 5), sel_metric:Optional[str]=None, show_plot:bool=False): \n",
    "        \n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            V_pct      : subset of train where OGR will be measured (to estimate L*)\n",
    "            n          : None: offline learning, int: super-epoch (online learning), tuple: (warmup super-epoch, super-epoch)(online learning with warm up)\n",
    "            sel_metric : which metric will be used to calculate overfitting and generalization during training. If None, loss will be used.\n",
    "            show_plot  : will show a plot with the wieghts at the end of training\n",
    "        \"\"\"\n",
    "        assert V_pct < 1, 'V_pct must be < 1'\n",
    "        self.V_pct, self.n, self.sel_metric, self.show_plot = V_pct, n, sel_metric, show_plot\n",
    "        self.metric_idx = None\n",
    "\n",
    "    def before_fit(self):\n",
    "        \n",
    "        # model\n",
    "        self.M = self.model.M \n",
    "        self.old_multi_output = self.learn.model.multi_output\n",
    "        self.learn.model.multi_output = True\n",
    "\n",
    "        #loss\n",
    "        if cls_name(self.learn.loss_func) != 'GBlendLoss': self.learn.loss_func = GBlendLoss(crit=self.learn.loss_func)\n",
    "\n",
    "        # calculate super_epochs\n",
    "        if self.n is None: \n",
    "            self.super_epochs = [0]\n",
    "        else: \n",
    "            if is_listy(self.n): \n",
    "                self.wu_n = self.n[0]\n",
    "                self.n = self.n[1]\n",
    "            else: \n",
    "                self.wu_n = self.n\n",
    "            rng = range(int(max(0, self.n_epoch - self.wu_n) / self.n + 1))\n",
    "            self.super_epochs = []\n",
    "            for i in rng: \n",
    "                self.super_epochs.append((i * self.wu_n) if i <= 1 else int((i + self.wu_n / self.n - 1) * self.n))\n",
    "        self.super_epochs.append(self.n_epoch)\n",
    "        \n",
    "        # create T'(Tp) and V dataloaders\n",
    "        n_out = len(self.learn.dls.train.dataset.ptls) - self.learn.dls.train.dataset.n_inp\n",
    "        train_targets = self.learn.dls.train.dataset.ptls[-n_out]\n",
    "        Tp_idx, V_idx = get_splits(train_targets, valid_size=self.V_pct)\n",
    "        _Tp_train_dls = []\n",
    "        _V_train_dls = []\n",
    "        self.learn.new_dls = []\n",
    "        for dl in self.learn.dls[0].loaders: # train MixedDataLoaders\n",
    "            _Tp_dl = get_subset_dl(dl, Tp_idx)\n",
    "            _V_dl = get_subset_dl(dl, V_idx)\n",
    "            _Tp_train_dls.append(_Tp_dl)\n",
    "            _V_train_dls.append(_V_dl) \n",
    "            self.learn.new_dls.append(DataLoaders(_Tp_dl, _V_dl, device=self.learn.dls.device))\n",
    "        self.learn.new_dls.append(MixedDataLoaders(MixedDataLoader(*_Tp_train_dls, shuffle=True),  # train - train\n",
    "                                                   MixedDataLoader(*_V_train_dls, shuffle=False),  # train - valid\n",
    "                                                   device=self.learn.dls.device))\n",
    "        \n",
    "        # prepare containers\n",
    "        self.learn.LT = []\n",
    "        self.learn.LV = []\n",
    "\n",
    "    def before_train(self):\n",
    "        if self.epoch in self.super_epochs[:-1] and not 'LRFinder' in [cls_name(cb) for cb in self.learn.cbs]: \n",
    "            self.train_epochs = np.diff(self.super_epochs)[self.super_epochs.index(self.epoch)]\n",
    "            \n",
    "            #compute weights\n",
    "            self.learn.save('gblend_learner')\n",
    "            torch.save(self.learn.model, 'gblend_model')\n",
    "            w = self.compute_weights()\n",
    "            if self.epoch == 0: self.learn.ws = [w]\n",
    "            else: self.learn.ws.append(w)\n",
    "            self.learn = self.learn.load('gblend_learner')\n",
    "            self.learn.loss_func.w = w\n",
    "\n",
    "    def compute_weights(self):\n",
    "\n",
    "        # _LT0 = []\n",
    "        # _LV0 = []\n",
    "        _LT = []\n",
    "        _LV = []\n",
    "        for i in range(self.M + 1):            \n",
    "            model = torch.load('gblend_model')\n",
    "            learn = Learner(self.learn.new_dls[i], model.m[i], loss_func=GBlendLoss(), \n",
    "                            opt_func=self.learn.opt_func, metrics=self.learn.metrics)\n",
    "            learn.model.multi_output = False\n",
    "            learn.remove_cbs(learn.cbs[1])\n",
    "            learn.add_cb(Recorder(train_metrics=True))\n",
    "            with learn.no_bar():\n",
    "                with learn.no_logging(): \n",
    "                    learn.fit_one_cycle(self.train_epochs, pct_start=0)\n",
    "            if self.metric_idx is None and self.sel_metric is not None:\n",
    "                metric_names = learn.recorder.metric_names[1:-1]\n",
    "                self.metric_idx = [i for i,m in enumerate(metric_names) if self.sel_metric in m]\n",
    "            else: self.metric_idx = [0, 1]\n",
    "            metric_values = learn.recorder.values[-1][self.metric_idx]\n",
    "            _LT.append(metric_values[0])\n",
    "            _LV.append(metric_values[1])\n",
    "\n",
    "        # if self.epoch == 0: self.compute_previous_metrics()\n",
    "        self.compute_previous_metrics()\n",
    "        self.learn.LT.append(_LT)\n",
    "        self.learn.LV.append(_LV)\n",
    "\n",
    "        LT1 = array(self.learn.LT[-2])\n",
    "        LT2 = array(self.learn.LT[-1])\n",
    "        LV1 = array(self.learn.LV[-2])\n",
    "        LV2 = array(self.learn.LV[-1])\n",
    "\n",
    "        Î”G = (LV1 - LV2) if self.metric_idx[0] == 0 else (LV2 - LV1)\n",
    "        O1 = (LV1 - LT1) if self.metric_idx[0] == 0 else (LT1 - LV1)\n",
    "        O2 = (LV2 - LT2) if self.metric_idx[0] == 0 else (LT2 - LV2)\n",
    "\n",
    "        Î”G = np.maximum(0, Î”G)\n",
    "\n",
    "        Î”O = O2 - O1\n",
    "        Î”O2 = np.maximum(1e-8, (O2 - O1)**2)\n",
    "        w = np.maximum(1e-8, np.nan_to_num(Î”G / Î”O2))\n",
    "        w = w / w.sum()\n",
    "        w = w.tolist()\n",
    "        return w\n",
    "\n",
    "    def compute_previous_metrics(self):\n",
    "        if self.metric_idx[0] == 0:  metric = self.loss_func\n",
    "        else: metric = self.learn.metrics[(min(array(self.metric_idx) - 2) - 1) // 2]\n",
    "        _LT = []\n",
    "        _LV = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.M + 1):\n",
    "                model = torch.load('gblend_model')\n",
    "                model.multi_output = False\n",
    "                model = model.m[i]\n",
    "                _train_metrics = []\n",
    "                _valid_metrics = []\n",
    "                for j,dl in enumerate(self.learn.new_dls[i]):\n",
    "                    it = iter(dl)\n",
    "                    _preds = []\n",
    "                    _targets = []\n",
    "                    for b in it: \n",
    "                        _preds.extend(model(*b[:-1]))\n",
    "                        _targets.extend(b[-1])\n",
    "                    _preds, _targets = stack(_preds), stack(_targets)\n",
    "                    try: _metric_values = metric(_preds, _targets).cpu().item()\n",
    "                    except: _metric_values = metric(torch.argmax(_preds, 1), _targets).cpu().item()\n",
    "                    if j == 0: _LT.append(_metric_values)\n",
    "                    else: _LV.append(_metric_values)\n",
    "            self.learn.LT.append(_LT)\n",
    "            self.learn.LV.append(_LV)\n",
    "\n",
    "    def after_fit(self):\n",
    "        if hasattr(self.learn, \"ws\") and self.show_plot:\n",
    "            widths = np.diff(self.super_epochs)\n",
    "            cum_ws = 0\n",
    "            for i in range(self.M + 1):\n",
    "                plt.bar(self.super_epochs[:-1] + widths/2, stack(self.learn.ws)[:, i], bottom=cum_ws, width=widths, \n",
    "                        label=f'k={i+1}' if i < self.M else f'fused')\n",
    "                cum_ws += stack(self.learn.ws)[:, i]\n",
    "            plt.xlim(0, self.super_epochs[-1])\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xticks(self.super_epochs)\n",
    "            plt.legend(loc='best')\n",
    "            plt.title('Online G-Blend Weights by modality')\n",
    "            plt.show()\n",
    "\n",
    "        self.learn.model.multi_output = self.old_multi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import *\n",
    "from tsai.data.all import *\n",
    "from tsai.models.utils import *\n",
    "from tsai.models.XCM import *\n",
    "from tsai.models.TabModel import *\n",
    "from tsai.models.MultiInputNet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X         : [samples=360 x features=24 x timesteps=51]\n",
      "y shape   : (360,)\n",
      "n classes : 6 ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0']\n",
      "n_splits  : [180, 180]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:04<00:00,  8.69it/s]\n"
     ]
    }
   ],
   "source": [
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "get_info(X,y,splits)\n",
    "ts_features_df = get_ts_features(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.795004</td>\n",
       "      <td>1.793661</td>\n",
       "      <td>0.272222</td>\n",
       "      <td>0.564630</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# raw ts\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = TSStandardize()\n",
    "ts_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "ts_model = build_ts_model(XCM, dls=ts_dls, window_perc=.5)\n",
    "\n",
    "# ts features\n",
    "cat_names = None\n",
    "cont_names = ts_features_df.columns[:-2]\n",
    "y_names = 'target'\n",
    "tab_dls = get_tabular_dls(ts_features_df, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits)\n",
    "tab_model = build_tabular_model(TabModel, dls=tab_dls)\n",
    "\n",
    "# mixed\n",
    "mixed_dls = get_mixed_dls(ts_dls, tab_dls)\n",
    "MultiModalNet = MultiInputNet(ts_model, tab_model, c_out=mixed_dls.c)\n",
    "gblend = GBlend(V_pct=.5, n=(10, 5), sel_metric=None)\n",
    "learn = Learner(mixed_dls, MultiModalNet, metrics=[accuracy, RocAuc()], cbs=gblend)\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 000c_data.preparation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 003_data.preprocessing.ipynb.\n",
      "Converted 003b_data.transforms.ipynb.\n",
      "Converted 003c_data.mixed_augmentation.ipynb.\n",
      "Converted 003d_data.image.ipynb.\n",
      "Converted 003e_data.features.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 006_data.mixed.ipynb.\n",
      "Converted 007_metrics.ipynb.\n",
      "Converted 008_learner.ipynb.\n",
      "Converted 009_optimizer.ipynb.\n",
      "Converted 010_callback.core.ipynb.\n",
      "Converted 011_callback.semi_supervised.ipynb.\n",
      "Converted 012_callback.gblend.ipynb.\n",
      "Converted 100_models.utils.ipynb.\n",
      "Converted 100b_models.layers.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/Documents/Machine_Learning/Jupyter_Notebooks/tsai/tsai\n",
      "Correct conversion! ðŸ˜ƒ\n",
      "Total time elapsed 182 s\n",
      "Monday 12/14/20 08:48:05 CET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
