{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: This contains metrics not included in fastai.\n",
    "output-file: metrics.html\n",
    "title: Metrics\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import sklearn.metrics as skm\n",
    "from fastai.metrics import * \n",
    "from tsai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "mk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},\n",
    "         doc=\"All possible activation classes for `AccumMetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MatthewsCorrCoefBinary(sample_weight=None):\n",
    "    \"Matthews correlation coefficient for single-label classification problems\"\n",
    "    return AccumMetric(skm.matthews_corrcoef, dim_argmax=-1, activation=ActivationType.BinarySoftmax, thresh=.5, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_task_metrics(dls, binary_metrics=None, multi_class_metrics=None, regression_metrics=None, verbose=True): \n",
    "    if dls.c == 2: \n",
    "        pv('binary-classification task', verbose)\n",
    "        return binary_metrics\n",
    "    elif dls.c > 2: \n",
    "        pv('multi-class task', verbose)\n",
    "        return multi_class_metrics\n",
    "    else:\n",
    "        pv('regression task', verbose)\n",
    "        return regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics applicable to multi classification have been created by Doug Williams (https://github.com/williamsdoug). Thanks a lot Doug!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True, by_sample=False):\n",
    "    \"Computes accuracy when `inp` and `targ` are the same size.\"\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    correct = (inp>thresh)==targ.bool()\n",
    "    if by_sample:\n",
    "        return (correct.float().mean(-1) == 1).float().mean()\n",
    "    else:\n",
    "        inp,targ = flatten_check(inp,targ)\n",
    "        return correct.float().mean()\n",
    "    \n",
    "def metrics_multi_common(inp, targ, thresh=0.5, sigmoid=True, by_sample=False):\n",
    "    \"Computes TP, TN, FP, FN when `inp` and `targ` are the same size.\"\n",
    "    if not by_sample: inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TP = torch.logical_and(correct, (targ==1).bool()).sum()\n",
    "    TN = torch.logical_and(correct, (targ==0).bool()).sum()\n",
    "    \n",
    "    incorrect = pred!=targ.bool()\n",
    "    FN = torch.logical_and(incorrect, (targ==1).bool()).sum()\n",
    "    FP = torch.logical_and(incorrect, (targ==0).bool()).sum()\n",
    "    \n",
    "    N =  targ.size()[0]\n",
    "    return N, TP, TN, FP, FN\n",
    "\n",
    "def precision_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    \"Computes precision when `inp` and `targ` are the same size.\"\n",
    "    \n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
    "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    return precision\n",
    "\n",
    "def recall_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    \"Computes recall when `inp` and `targ` are the same size.\"\n",
    "    \n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
    "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
    "\n",
    "    recall = TP/(TP+FN)\n",
    "    return recall\n",
    "\n",
    "def specificity_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    \"Computes specificity (true negative rate) when `inp` and `targ` are the same size.\"\n",
    "    \n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TN = torch.logical_and(correct,  (targ==0).bool()).sum()\n",
    "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
    "\n",
    "    specificity = TN/(TN+FP)\n",
    "    return specificity\n",
    "\n",
    "def balanced_accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    \"Computes balanced accuracy when `inp` and `targ` are the same size.\"\n",
    "    \n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
    "    TN = torch.logical_and(correct,  (targ==0).bool()).sum()\n",
    "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
    "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
    "\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    balanced_accuracy = (TPR+TNR)/2\n",
    "    return balanced_accuracy\n",
    "\n",
    "def Fbeta_multi(inp, targ, beta=1.0, thresh=0.5, sigmoid=True):\n",
    "    \"Computes Fbeta when `inp` and `targ` are the same size.\"\n",
    "    \n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    pred = inp>thresh\n",
    "    \n",
    "    correct = pred==targ.bool()\n",
    "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
    "    TN = torch.logical_and(correct,  (targ==0).bool()).sum()\n",
    "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
    "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    beta2 = beta*beta\n",
    "    \n",
    "    if precision+recall > 0:\n",
    "        Fbeta = (1+beta2)*precision*recall/(beta2*precision+recall)\n",
    "    else:\n",
    "        Fbeta = 0\n",
    "    return Fbeta\n",
    "\n",
    "def F1_multi(*args, **kwargs):\n",
    "    return Fbeta_multi(*args, **kwargs)  # beta defaults to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def mae(inp,targ):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return torch.abs(inp - targ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def mape(inp,targ):\n",
    "    \"Mean absolute percentage error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp, targ)\n",
    "    return (torch.abs(inp - targ) / torch.clamp_min(targ, 1e-8)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _recall_at_specificity(inp, targ, specificity=.95, axis=-1):\n",
    "    inp0 = inp[(targ == 0).data]\n",
    "    inp1 = inp[(targ == 1).data]\n",
    "    thr = torch.sort(inp0).values[-int(len(inp0) * (1 - specificity))]\n",
    "    return (inp1 > thr).float().mean()\n",
    "\n",
    "recall_at_specificity = AccumMetric(_recall_at_specificity, specificity=.95, activation=ActivationType.BinarySoftmax, \n",
    "                                    flatten=False, name='recall_at_specificity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _lift(inp, targ, axis=-1):\n",
    "    \"Calculates lift as precision / average rate\"\n",
    "    return targ[(torch.argmax(inp, -1) == 1).data].mean() / targ.mean()\n",
    "\n",
    "lift = AccumMetric(_lift, activation=ActivationType.BinarySoftmax, flatten=False, name='lift')\n",
    "\n",
    "def _lift_at_specificity(inp, targ, specificity=0.95, axis=-1):\n",
    "    \"Calculates lift as precision / average rate at a given specificity\"\n",
    "    inp0 = inp[(targ == 0).data]\n",
    "    thr = torch.sort(inp0).values[-int(len(inp0) * (1 - specificity))]\n",
    "    return (targ[(inp >= thr).data] == 1).float().mean() / (targ == 1).float().mean()\n",
    "\n",
    "lift_at_specificity = AccumMetric(_lift_at_specificity, specificity=.95, activation=ActivationType.BinarySoftmax, \n",
    "                                  flatten=False, name='lift_at_specificity')\n",
    "\n",
    "def _top_k_lift(inp, targ, k=0.01):\n",
    "    \"\"\"Top k over random k lift calculated as the ratio between precision at \n",
    "    top k % positive probabilities and average ratio\"\"\"\n",
    "    top_k_thr = torch.sort(inp).values[-int(k * len(inp))]\n",
    "    return targ[(inp >= top_k_thr).data].float().mean() / (targ == 1).float().mean()\n",
    "\n",
    "top_k_lift = AccumMetric(_top_k_lift, k=.01, activation=ActivationType.BinarySoftmax, flatten=False, name='top_k_lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _mean_per_class_accuracy(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None):\n",
    "    cm = skm.confusion_matrix(y_true, y_pred, labels=labels, sample_weight=sample_weight, normalize=normalize)\n",
    "    return (cm.diagonal() / cm.sum(1)).mean()\n",
    "\n",
    "mean_per_class_accuracy = skm_to_fastai(_mean_per_class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from tsai.imports import create_scripts\n",
    "from tsai.export import get_nb_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "nb_name = get_nb_name()\n",
    "create_scripts(nb_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37torch110')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
